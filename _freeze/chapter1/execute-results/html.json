{
  "hash": "715e25a48085acd461871250d4febe85",
  "result": {
    "markdown": "# 第１章:強化学習とは\n\n**強化学習**は機械学習の一種で、環境とエージェントの**相互作用**に注目した手法である。\n\n教師あり/なし学習と違い、与えられた最適解（もしくは隠れた最適解）を探すのではなく、最も大きい報酬をもたらす行動を自分で見つけ出す。また、あるタイムステップでの行動はその時点での報酬だけではなく、その後の報酬にも影響を与える。よって、**探索**と**遅延報酬**という二つの特性を持つ。\n\n## 具体的な構成要素\n\n|                          |                                                                                |\n|--------------------------|--------------------------------------------------------------------------------|\n| 方策(policy)             | ある状態でのそこからとるべき行動への写像                                       |\n| 報酬(reward)             | 各時間ステップごとの行動を評価して環境からエージェントに送られるもの。         |\n| 価値関数(value function) | 報酬とは違い、その状態の**長期的な価値**を導く。                               |\n| モデル(model)            | 環境の挙動を模倣する、主に未来の環境（状態遷移と報酬）を予測するための関数群。 |\n\n強化学習の最終目標はこの報酬を最大化することである。\n\n４つ目の環境モデルは必須要素ではなく、使用したものを**モデルベース**、していないものを**モデルフリー**手法と呼ぶ。\n\n## 進化的手法との違い\n\n（当書では別のものとして捉えているが、進化的手法を強化学習アルゴリズムの一派とする考え方もある。）\n\n遺伝的アルゴリズム、遺伝的プログラミング、焼きなまし法などを**進化的**手法と呼ぶ。これらは生物のように優れたエージェントの特徴を引き継いで次の世代が獲得する報酬を大きくする。\n\n環境からの情報をもとに行動し、報酬を最大化するという点で強化学習と似ているが、最初に述べた環境との相互作用がないという点で異なる。\n\nつまり、進化的手法ではどの状態でどの行動を取ったのか、という情報を使わない。エージェントが環境の状態を知覚しづらい（数値化が難しい）場合などを除いて、強化学習のほうが効率的に学習できることが多い。\n\n## シンプルな例：三目並べ\n\n強化学習を用いて三目並べに勝つことを目標とするエージェントを作る。\n\n以下のような行動木を作成する。それぞれの状態（盤面）には0\\~1の価値が設定されている。どうやっても負ける状態には0、既に勝っている状態には1、それ以外には0.5が初期値として割り当てられる。\n\n学習中、基本的には価値が大きい、勝率が高い手を選ぶが、すべての状態を試すためにランダム性を持った行動選択（**探索的な手**）をさせる。\n\n状態の価値の更新は以下の式で行う。\n\n\n\n$$\nV(s_t) \\leftarrow V(s_t)+\\alpha\\left[V\\left(s_{t+1}\\right)-V(s_t)\\right]\n$$\n\n\n\n$V(s)$は状態$s$の推定価値を表し、次の手を選択した際、次の状態の価値に近づけている。ステップサイズパラメータ$\\alpha$によって学習率を調整している。\n\nこの更新則は**TD学習**（時間差分学習; temporal-difference learning）と呼ばれる。 \n\n## 練習問題\n\n1.1 自己対戦（self play）\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\n    '''\n    import numpy as np\n    import pickle\n    '''\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n'\\nimport numpy as np\\nimport pickle\\n'\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\n    \"\"\"\n\n    BOARD_ROWS = 3\n    BOARD_COLS = 3\n    BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n\n    class State:\n        def __init__(self):\n            self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n            self.winner = None\n            self.hashVal = None\n            self.end = None\n\n        def getHash(self):\n            if self.hashVal is None:\n                self.hashVal = 0\n                for i in self.data.reshape(BOARD_ROWS * BOARD_COLS):\n                    if i == -1:\n                        i = 2\n                    self.hashVal = self.hashVal * 3 + i\n            return int(self.hashVal)\n\n        def isEnd(self):\n            if self.end is not None:\n                return self.end\n            results = []\n            for i in range(0, BOARD_ROWS):\n                results.append(np.sum(self.data[i, :]))\n            for i in range(0, BOARD_COLS):\n                results.append(np.sum(self.data[:, i]))\n\n            results.append(0)\n            for i in range(0, BOARD_ROWS):\n                results[-1] += self.data[i, i]\n            results.append(0)\n            for i in range(0, BOARD_ROWS):\n                results[-1] += self.data[i, BOARD_ROWS - 1 - i]\n\n            for result in results:\n                if result == 3:\n                    self.winner = 1\n                    self.end = True\n                    return self.end\n                if result == -3:\n                    self.winner = -1\n                    self.end = True\n                    return self.end\n\n            sum = np.sum(np.abs(self.data))\n            if sum == BOARD_ROWS * BOARD_COLS:\n                self.winner = 0\n                self.end = True\n                return self.end\n\n            self.end = False\n            return self.end\n\n        def nextState(self, i, j, symbol):\n            newState = State()\n            newState.data = np.copy(self.data)\n            newState.data[i, j] = symbol\n            return newState\n\n        # print board\n        def show(self):\n            for i in range(0, BOARD_ROWS):\n                print('-------------')\n                out = '| '\n                for j in range(0, BOARD_COLS):\n                    if self.data[i, j] == 1:\n                        token = '*'\n                    if self.data[i, j] == 0:\n                        token = '0'\n                    if self.data[i, j] == -1:\n                        token = 'x'\n                    out += token + ' | '\n                print(out)\n            print('-------------')\n\n    def getAllStatesImpl(currentState, currentSymbol, allStates):\n        for i in range(0, BOARD_ROWS):\n            for j in range(0, BOARD_COLS):\n                if currentState.data[i][j] == 0:\n                    newState = currentState.nextState(i, j, currentSymbol)\n                    newHash = newState.getHash()\n                    if newHash not in allStates.keys():\n                        isEnd = newState.isEnd()\n                        allStates[newHash] = (newState, isEnd)\n                        if not isEnd:\n                            getAllStatesImpl(newState, -currentSymbol, allStates)\n\n    def getAllStates():\n        currentSymbol = 1\n        currentState = State()\n        allStates = dict()\n        allStates[currentState.getHash()] = (currentState, currentState.isEnd())\n        getAllStatesImpl(currentState, currentSymbol, allStates)\n        return allStates\n\n    allStates = getAllStates()\n\n    class Judger:\n        def __init__(self, player1, player2, feedback=True):\n            self.p1 = player1\n            self.p2 = player2\n            self.feedback = feedback\n            self.currentPlayer = None\n            self.p1Symbol = 1\n            self.p2Symbol = -1\n            self.p1.setSymbol(self.p1Symbol)\n            self.p2.setSymbol(self.p2Symbol)\n            self.currentState = State()\n            self.allStates = allStates\n\n        def giveReward(self):\n            if self.currentState.winner == self.p1Symbol:\n                self.p1.feedReward(1)\n                self.p2.feedReward(0)\n            elif self.currentState.winner == self.p2Symbol:\n                self.p1.feedReward(0)\n                self.p2.feedReward(1)\n            else:\n                self.p1.feedReward(0.1)\n                self.p2.feedReward(0.5)\n\n        def feedCurrentState(self):\n            self.p1.feedState(self.currentState)\n            self.p2.feedState(self.currentState)\n\n        def reset(self):\n            self.p1.reset()\n            self.p2.reset()\n            self.currentState = State()\n            self.currentPlayer = None\n\n        def play(self, show=False):\n            self.reset()\n            self.feedCurrentState()\n            while True:\n                if self.currentPlayer == self.p1:\n                    self.currentPlayer = self.p2\n                else:\n                    self.currentPlayer = self.p1\n                if show:\n                    self.currentState.show()\n                [i, j, symbol] = self.currentPlayer.takeAction()\n                self.currentState = self.currentState.nextState(i, j, symbol)\n                hashValue = self.currentState.getHash()\n                self.currentState, isEnd = self.allStates[hashValue]\n                self.feedCurrentState()\n                if isEnd:\n                    if self.feedback:\n                        self.giveReward()\n                    return self.currentState.winner\n\n    # AI player\n    class Player:\n        def __init__(self, stepSize = 0.1, exploreRate=0.1):\n            self.allStates = allStates\n            self.estimations = dict()\n            self.stepSize = stepSize\n            self.exploreRate = exploreRate\n            self.states = []\n\n        def reset(self):\n            self.states = []\n\n        def setSymbol(self, symbol):\n            self.symbol = symbol\n            for hash in self.allStates.keys():\n                (state, isEnd) = self.allStates[hash]\n                if isEnd:\n                    if state.winner == self.symbol:\n                        self.estimations[hash] = 1.0\n                    else:\n                        self.estimations[hash] = 0\n                else:\n                    self.estimations[hash] = 0.5\n\n        def feedState(self, state):\n            self.states.append(state)\n\n        def feedReward(self, reward):\n            if len(self.states) == 0:\n                return\n            self.states = [state.getHash() for state in self.states]\n            target = reward\n            for latestState in reversed(self.states):\n                value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState])\n                self.estimations[latestState] = value\n                target = value\n            self.states = []\n\n        def takeAction(self):\n            state = self.states[-1]\n            nextStates = []\n            nextPositions = []\n            for i in range(BOARD_ROWS):\n                for j in range(BOARD_COLS):\n                    if state.data[i, j] == 0:\n                        nextPositions.append([i, j])\n                        nextStates.append(state.nextState(i, j, self.symbol).getHash())\n            if np.random.binomial(1, self.exploreRate):\n                np.random.shuffle(nextPositions)\n                self.states = []\n                action = nextPositions[0]\n                action.append(self.symbol)\n                return action\n\n            values = []\n            for hash, pos in zip(nextStates, nextPositions):\n                values.append((self.estimations[hash], pos))\n            np.random.shuffle(values)\n            values.sort(key=lambda x: x[0], reverse=True)\n            action = values[0][1]\n            action.append(self.symbol)\n            return action\n\n        def savePolicy(self):\n            fw = open('optimal_policy_' + str(self.symbol), 'wb')\n            pickle.dump(self.estimations, fw)\n            fw.close()\n\n        def loadPolicy(self):\n            fr = open('optimal_policy_' + str(self.symbol),'rb')\n            self.estimations = pickle.load(fr)\n            fr.close()\n            \n    # | 1 | 2 | 3 |\n    # | 4 | 5 | 6 |\n    # | 7 | 8 | 9 |\n    class HumanPlayer:\n        def __init__(self, stepSize = 0.1, exploreRate=0.1):\n            self.symbol = None\n            self.currentState = None\n            return\n        def reset(self):\n            return\n        def setSymbol(self, symbol):\n            self.symbol = symbol\n            return\n        def feedState(self, state):\n            self.currentState = state\n            return\n        def feedReward(self, reward):\n            return\n        def takeAction(self):\n            data = int(input(\"Input your position:\"))\n            data -= 1\n            i = data // int(BOARD_COLS)\n            j = data % BOARD_COLS\n            if self.currentState.data[i, j] != 0:\n                return self.takeAction()\n            return (i, j, self.symbol)\n\n    def train(epochs=20000):\n        player1 = Player()\n        player2 = Player()\n        judger = Judger(player1, player2)\n        player1Win = 0.0\n        player2Win = 0.0\n        for i in range(0, epochs):\n            print(\"Epoch\", i)\n            winner = judger.play()\n            if winner == 1:\n                player1Win += 1\n            if winner == -1:\n                player2Win += 1\n            judger.reset()\n        print(player1Win / epochs)\n        print(player2Win / epochs)\n        player1.savePolicy()\n        player2.savePolicy()\n\n    def compete(turns=500):\n        player1 = Player(exploreRate=0)\n        player2 = Player(exploreRate=0)\n        judger = Judger(player1, player2, False)\n        player1.loadPolicy()\n        player2.loadPolicy()\n        player1Win = 0.0\n        player2Win = 0.0\n        for i in range(0, turns):\n            print(\"Epoch\", i)\n            winner = judger.play()\n            if winner == 1:\n                player1Win += 1\n            if winner == -1:\n                player2Win += 1\n            judger.reset()\n        print(player1Win / turns)\n        print(player2Win / turns)\n\n    def play():\n        while True:\n            player1 = Player(exploreRate=0)\n            player2 = HumanPlayer()\n            judger = Judger(player1, player2, False)\n            player1.loadPolicy()\n            winner = judger.play(True)\n            if winner == player2.symbol:\n                print(\"Win!\")\n            elif winner == player1.symbol:\n                print(\"Lose!\")\n            else:\n                print(\"Tie!\")\n\n    train()\n    compete()\n    play()\n    \"\"\"\n```\n\n````\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n'\\n\\nBOARD_ROWS = 3\\nBOARD_COLS = 3\\nBOARD_SIZE = BOARD_ROWS * BOARD_COLS\\n\\nclass State:\\n    def __init__(self):\\n        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\\n        self.winner = None\\n        self.hashVal = None\\n        self.end = None\\n\\n    def getHash(self):\\n        if self.hashVal is None:\\n            self.hashVal = 0\\n            for i in self.data.reshape(BOARD_ROWS * BOARD_COLS):\\n                if i == -1:\\n                    i = 2\\n                self.hashVal = self.hashVal * 3 + i\\n        return int(self.hashVal)\\n\\n    def isEnd(self):\\n        if self.end is not None:\\n            return self.end\\n        results = []\\n        for i in range(0, BOARD_ROWS):\\n            results.append(np.sum(self.data[i, :]))\\n        for i in range(0, BOARD_COLS):\\n            results.append(np.sum(self.data[:, i]))\\n\\n        results.append(0)\\n        for i in range(0, BOARD_ROWS):\\n            results[-1] += self.data[i, i]\\n        results.append(0)\\n        for i in range(0, BOARD_ROWS):\\n            results[-1] += self.data[i, BOARD_ROWS - 1 - i]\\n\\n        for result in results:\\n            if result == 3:\\n                self.winner = 1\\n                self.end = True\\n                return self.end\\n            if result == -3:\\n                self.winner = -1\\n                self.end = True\\n                return self.end\\n\\n        sum = np.sum(np.abs(self.data))\\n        if sum == BOARD_ROWS * BOARD_COLS:\\n            self.winner = 0\\n            self.end = True\\n            return self.end\\n\\n        self.end = False\\n        return self.end\\n\\n    def nextState(self, i, j, symbol):\\n        newState = State()\\n        newState.data = np.copy(self.data)\\n        newState.data[i, j] = symbol\\n        return newState\\n\\n    # print board\\n    def show(self):\\n        for i in range(0, BOARD_ROWS):\\n            print(\\'-------------\\')\\n            out = \\'| \\'\\n            for j in range(0, BOARD_COLS):\\n                if self.data[i, j] == 1:\\n                    token = \\'*\\'\\n                if self.data[i, j] == 0:\\n                    token = \\'0\\'\\n                if self.data[i, j] == -1:\\n                    token = \\'x\\'\\n                out += token + \\' | \\'\\n            print(out)\\n        print(\\'-------------\\')\\n\\ndef getAllStatesImpl(currentState, currentSymbol, allStates):\\n    for i in range(0, BOARD_ROWS):\\n        for j in range(0, BOARD_COLS):\\n            if currentState.data[i][j] == 0:\\n                newState = currentState.nextState(i, j, currentSymbol)\\n                newHash = newState.getHash()\\n                if newHash not in allStates.keys():\\n                    isEnd = newState.isEnd()\\n                    allStates[newHash] = (newState, isEnd)\\n                    if not isEnd:\\n                        getAllStatesImpl(newState, -currentSymbol, allStates)\\n\\ndef getAllStates():\\n    currentSymbol = 1\\n    currentState = State()\\n    allStates = dict()\\n    allStates[currentState.getHash()] = (currentState, currentState.isEnd())\\n    getAllStatesImpl(currentState, currentSymbol, allStates)\\n    return allStates\\n\\nallStates = getAllStates()\\n\\nclass Judger:\\n    def __init__(self, player1, player2, feedback=True):\\n        self.p1 = player1\\n        self.p2 = player2\\n        self.feedback = feedback\\n        self.currentPlayer = None\\n        self.p1Symbol = 1\\n        self.p2Symbol = -1\\n        self.p1.setSymbol(self.p1Symbol)\\n        self.p2.setSymbol(self.p2Symbol)\\n        self.currentState = State()\\n        self.allStates = allStates\\n\\n    def giveReward(self):\\n        if self.currentState.winner == self.p1Symbol:\\n            self.p1.feedReward(1)\\n            self.p2.feedReward(0)\\n        elif self.currentState.winner == self.p2Symbol:\\n            self.p1.feedReward(0)\\n            self.p2.feedReward(1)\\n        else:\\n            self.p1.feedReward(0.1)\\n            self.p2.feedReward(0.5)\\n\\n    def feedCurrentState(self):\\n        self.p1.feedState(self.currentState)\\n        self.p2.feedState(self.currentState)\\n\\n    def reset(self):\\n        self.p1.reset()\\n        self.p2.reset()\\n        self.currentState = State()\\n        self.currentPlayer = None\\n\\n    def play(self, show=False):\\n        self.reset()\\n        self.feedCurrentState()\\n        while True:\\n            if self.currentPlayer == self.p1:\\n                self.currentPlayer = self.p2\\n            else:\\n                self.currentPlayer = self.p1\\n            if show:\\n                self.currentState.show()\\n            [i, j, symbol] = self.currentPlayer.takeAction()\\n            self.currentState = self.currentState.nextState(i, j, symbol)\\n            hashValue = self.currentState.getHash()\\n            self.currentState, isEnd = self.allStates[hashValue]\\n            self.feedCurrentState()\\n            if isEnd:\\n                if self.feedback:\\n                    self.giveReward()\\n                return self.currentState.winner\\n\\n# AI player\\nclass Player:\\n    def __init__(self, stepSize = 0.1, exploreRate=0.1):\\n        self.allStates = allStates\\n        self.estimations = dict()\\n        self.stepSize = stepSize\\n        self.exploreRate = exploreRate\\n        self.states = []\\n\\n    def reset(self):\\n        self.states = []\\n\\n    def setSymbol(self, symbol):\\n        self.symbol = symbol\\n        for hash in self.allStates.keys():\\n            (state, isEnd) = self.allStates[hash]\\n            if isEnd:\\n                if state.winner == self.symbol:\\n                    self.estimations[hash] = 1.0\\n                else:\\n                    self.estimations[hash] = 0\\n            else:\\n                self.estimations[hash] = 0.5\\n\\n    def feedState(self, state):\\n        self.states.append(state)\\n\\n    def feedReward(self, reward):\\n        if len(self.states) == 0:\\n            return\\n        self.states = [state.getHash() for state in self.states]\\n        target = reward\\n        for latestState in reversed(self.states):\\n            value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState])\\n            self.estimations[latestState] = value\\n            target = value\\n        self.states = []\\n\\n    def takeAction(self):\\n        state = self.states[-1]\\n        nextStates = []\\n        nextPositions = []\\n        for i in range(BOARD_ROWS):\\n            for j in range(BOARD_COLS):\\n                if state.data[i, j] == 0:\\n                    nextPositions.append([i, j])\\n                    nextStates.append(state.nextState(i, j, self.symbol).getHash())\\n        if np.random.binomial(1, self.exploreRate):\\n            np.random.shuffle(nextPositions)\\n            self.states = []\\n            action = nextPositions[0]\\n            action.append(self.symbol)\\n            return action\\n\\n        values = []\\n        for hash, pos in zip(nextStates, nextPositions):\\n            values.append((self.estimations[hash], pos))\\n        np.random.shuffle(values)\\n        values.sort(key=lambda x: x[0], reverse=True)\\n        action = values[0][1]\\n        action.append(self.symbol)\\n        return action\\n\\n    def savePolicy(self):\\n        fw = open(\\'optimal_policy_\\' + str(self.symbol), \\'wb\\')\\n        pickle.dump(self.estimations, fw)\\n        fw.close()\\n\\n    def loadPolicy(self):\\n        fr = open(\\'optimal_policy_\\' + str(self.symbol),\\'rb\\')\\n        self.estimations = pickle.load(fr)\\n        fr.close()\\n        \\n# | 1 | 2 | 3 |\\n# | 4 | 5 | 6 |\\n# | 7 | 8 | 9 |\\nclass HumanPlayer:\\n    def __init__(self, stepSize = 0.1, exploreRate=0.1):\\n        self.symbol = None\\n        self.currentState = None\\n        return\\n    def reset(self):\\n        return\\n    def setSymbol(self, symbol):\\n        self.symbol = symbol\\n        return\\n    def feedState(self, state):\\n        self.currentState = state\\n        return\\n    def feedReward(self, reward):\\n        return\\n    def takeAction(self):\\n        data = int(input(\"Input your position:\"))\\n        data -= 1\\n        i = data // int(BOARD_COLS)\\n        j = data % BOARD_COLS\\n        if self.currentState.data[i, j] != 0:\\n            return self.takeAction()\\n        return (i, j, self.symbol)\\n\\ndef train(epochs=20000):\\n    player1 = Player()\\n    player2 = Player()\\n    judger = Judger(player1, player2)\\n    player1Win = 0.0\\n    player2Win = 0.0\\n    for i in range(0, epochs):\\n        print(\"Epoch\", i)\\n        winner = judger.play()\\n        if winner == 1:\\n            player1Win += 1\\n        if winner == -1:\\n            player2Win += 1\\n        judger.reset()\\n    print(player1Win / epochs)\\n    print(player2Win / epochs)\\n    player1.savePolicy()\\n    player2.savePolicy()\\n\\ndef compete(turns=500):\\n    player1 = Player(exploreRate=0)\\n    player2 = Player(exploreRate=0)\\n    judger = Judger(player1, player2, False)\\n    player1.loadPolicy()\\n    player2.loadPolicy()\\n    player1Win = 0.0\\n    player2Win = 0.0\\n    for i in range(0, turns):\\n        print(\"Epoch\", i)\\n        winner = judger.play()\\n        if winner == 1:\\n            player1Win += 1\\n        if winner == -1:\\n            player2Win += 1\\n        judger.reset()\\n    print(player1Win / turns)\\n    print(player2Win / turns)\\n\\ndef play():\\n    while True:\\n        player1 = Player(exploreRate=0)\\n        player2 = HumanPlayer()\\n        judger = Judger(player1, player2, False)\\n        player1.loadPolicy()\\n        winner = judger.play(True)\\n        if winner == player2.symbol:\\n            print(\"Win!\")\\n        elif winner == player1.symbol:\\n            print(\"Lose!\")\\n        else:\\n            print(\"Tie!\")\\n\\ntrain()\\ncompete()\\nplay()\\n'\n```\n:::\n:::\n\n\n",
    "supporting": [
      "chapter1_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}