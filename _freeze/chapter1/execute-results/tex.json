{
  "hash": "cd02cb8243c1c663ba175a52d5f353b9",
  "result": {
    "markdown": "# 第１章:強化学習とは\n\n**強化学習**は機械学習の一種で、環境とエージェントの**相互作用**に注目した手法ある。\n\n教師あり/なし学習と違い、与えられた最適解（もしくは隠れた最適解）を探すのではなく、最も大きい報酬をもたらす行動を自分で見つけ出す。また、あるタイムステップでの行動はその時点での報酬だけではなく、その後の報酬にも影響を与える。よって、**探索**と**遅延報酬**という二つの特性を持つ。\n\n## 具体的な構成要素\n\n|                          |                                                                                |\n|--------------------------|--------------------------------------------------------------------------------|\n| 方策(policy)             | ある状態でのそこからとるべき行動への写像                                       |\n| 報酬(reward)             | 各時間ステップごとの行動を評価して環境からエージェントに送られるもの。         |\n| 価値関数(value function) | 報酬とは違い、その状態の**長期的な価値**を導く。                               |\n| モデル(model)            | 環境の挙動を模倣する、主に未来の環境（状態遷移と報酬）を予測するための関数群。 |\n\n強化学習の最終目標はこの報酬を最大化することである。\n\n４つ目の環境モデルは必須要素ではなく、使用したものを**モデルベース**、していないものを**モデルフリー**手法と呼ぶ。\n\n## 進化的手法との違い\n\n遺伝的アルゴリズム、遺伝的プログラミング、焼きなまし法などを**進化的**手法と呼ぶ。これらは生物のように優れたエージェントの特徴を引き継いで次の世代が獲得する報酬を大きくする。\n\n環境からの情報をもとに行動し、報酬を最大化するという点で強化学習と似ているが、最初に述べた環境との相互作用がないという点で異なる。<small>当書では別のものとして捉えているが、進化的手法を強化学習アルゴリズムの一派とする考え方もある。</small>\n\n\n\n$$\nV(s) \\leftarrow V(s)+\\alpha\\left[V\\left(s^{\\prime}\\right)-V(s)\\right]\n$$\n\n",
    "supporting": [
      "chapter1_files/figure-pdf"
    ],
    "filters": []
  }
}