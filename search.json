[
  {
    "objectID": "index.html#輪読会にあたって",
    "href": "index.html#輪読会にあたって",
    "title": "強化学習 輪読会",
    "section": "輪読会にあたって",
    "text": "輪読会にあたって\n第１章以降、とくに第Ⅱ部（第９章〜）からはそれぞれが独立した内容が多い。\n最初にすべての難易度を把握するのは難しいので、毎週次回の担当範囲を決めることとする。\n本の内容に沿って解説していき、わからないところがあれば聞き手はよこやりを入れて質問をしてよい。\n担当者はすべての質問に答えられるように準備する。"
  },
  {
    "objectID": "index.html#当書で使われる記号",
    "href": "index.html#当書で使われる記号",
    "title": "強化学習 輪読会",
    "section": "当書で使われる記号",
    "text": "当書で使われる記号\n\n1 + 1\n\n2"
  },
  {
    "objectID": "index.html#今後の進め方",
    "href": "index.html#今後の進め方",
    "title": "強化学習 輪読会",
    "section": "今後の進め方",
    "text": "今後の進め方\n二人で共通の文書を編集する形式に賛成であれば、次回までにこの文書をサーバーにアップロードし、峻平への導入方法を考えておく。\n\n\n\n日時\nろく\n峻平\n\n\n\n\n11/30\n序章\n\n\n\n12/5\n第一章\n\n\n\n12/12\n第二章前半？\n第二章後半？\n\n\n12/19\n\n\n\n\n12/26\n\n\n\n\n1/2\n\n\n\n\n\nそうでなければ、例えば一人はbook形式、もう一人はスライドを用いて発表する。"
  },
  {
    "objectID": "index.html#文書の編集方法",
    "href": "index.html#文書の編集方法",
    "title": "強化学習 輪読会",
    "section": "文書の編集方法",
    "text": "文書の編集方法\n\ngit環境を整える。（wslがおすすめ）\nrstudioとquartoをインストール\nhttps://github.com/Roku-3/rindoku_RLにアクセスし、mainブランチでリポジトリをクローンする。rstudioで開く。\n\n\n編集内容を適応する\ngitの基本コマンドは以下の通り。\ngit pull origin HEAD            # githubからローカルに差分をもってくる\n\ngit add .                       # 全てのファイルをコミット対象にする\n\ngit commit -m \"edit chapter 2\"  # 編集内容を表すコメント付きでコミットする。\n                                # -mはコミットメッセージを一行にするという意味\n                                \ngit push origin HEAD            # githubに情報を送る\n2人が同じ場所を変更していた場合、コンフリクトが発生する。その時は必ず解消してからpushを行う。\ngit branchと入力したときにmainになっていることを確認してからpushすること。\npushが正常に行われると自動でデプロイ（サイトが公開）される。\n\n\n今後の運用\n一人が文書を変更して適応した後、細かいミスなどで再編集するのは手間。また間違ってファイルを削除してしまった時の危険性も高い。\nそのため、後々はプルリクエストを出してもう一人が確認する形式にしようと思う。\nプルリクエストは手軽に人の編集を確認・訂正できるgithubの機能である。"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "序章のまとめ",
    "section": "",
    "text": "第１版序文\n強化学習は197X年ごろから存在が知られてきた。\nしかし、筆者は環境からどのようにして学習するか、ということにさほど注目されていないことに気がついた。"
  },
  {
    "objectID": "chapter1.html#具体的な構成要素",
    "href": "chapter1.html#具体的な構成要素",
    "title": "第１章:強化学習とは",
    "section": "具体的な構成要素",
    "text": "具体的な構成要素\n\n\n\n\n\n\n\n方策(policy)\nある状態でのそこからとるべき行動への写像\n\n\n報酬(reward)\n各時間ステップごとの行動を評価して環境からエージェントに送られるもの。\n\n\n価値関数(value function)\n報酬とは違い、その状態の長期的な価値を導く。\n\n\nモデル(model)\n環境の挙動を模倣する、主に未来の環境（状態遷移と報酬）を予測するための関数群。\n\n\n\n強化学習の最終目標はこの報酬を最大化することである。\n４つ目の環境モデルは必須要素ではなく、使用したものをモデルベース、していないものをモデルフリー手法と呼ぶ。"
  },
  {
    "objectID": "chapter1.html#進化的手法との違い",
    "href": "chapter1.html#進化的手法との違い",
    "title": "第１章:強化学習とは",
    "section": "進化的手法との違い",
    "text": "進化的手法との違い\n（当書では別のものとして捉えているが、進化的手法を強化学習アルゴリズムの一派とする考え方もある。）\n遺伝的アルゴリズム、遺伝的プログラミング、焼きなまし法などを進化的手法と呼ぶ。これらは生物のように優れたエージェントの特徴を引き継いで次の世代が獲得する報酬を大きくする。\n環境からの情報をもとに行動し、報酬を最大化するという点で強化学習と似ているが、最初に述べた環境との相互作用がないという点で異なる。\nつまり、進化的手法ではどの状態でどの行動を取ったのか、という情報を使わない。エージェントが環境の状態を知覚しづらい（数値化が難しい）場合などを除いて、強化学習のほうが効率的に学習できることが多い。"
  },
  {
    "objectID": "chapter1.html#シンプルな例三目並べ",
    "href": "chapter1.html#シンプルな例三目並べ",
    "title": "第１章:強化学習とは",
    "section": "シンプルな例：三目並べ",
    "text": "シンプルな例：三目並べ\n強化学習を用いて三目並べに勝つことを目標とするエージェントを作る。\n以下のような行動木を作成する。それぞれの状態（盤面）には0~1の価値が設定されている。どうやっても負ける状態には0、既に勝っている状態には1、それ以外には0.5が初期値として割り当てられる。\n\n学習中、基本的には価値が大きい、勝率が高い手を選ぶが、すべての状態を試すためにランダム性を持った行動選択（探索的な手）をさせる。\n状態の価値の更新は以下の式で行う。\n\\[\nV(s_t) \\leftarrow V(s_t)+\\alpha\\left[V\\left(s_{t+1}\\right)-V(s_t)\\right]\n\\]\n\\(V(s)\\)は状態\\(s\\)の推定価値を表し、次の手を選択した際、次の状態の価値に近づけている。ステップサイズパラメータ\\(\\alpha\\)によって学習率を調整している。\nこの更新則はTD学習（時間差分学習; temporal-difference learning）と呼ばれる。"
  },
  {
    "objectID": "chapter1.html#練習問題",
    "href": "chapter1.html#練習問題",
    "title": "第１章:強化学習とは",
    "section": "練習問題",
    "text": "練習問題\n1.1 自己対戦（self play）\n\n```{python}\n    '''\n    import numpy as np\n    import pickle\n    '''\n```\n\n'\\nimport numpy as np\\nimport pickle\\n'\n\n\n\n```{python}\n    \"\"\"\n\n    BOARD_ROWS = 3\n    BOARD_COLS = 3\n    BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n\n    class State:\n        def __init__(self):\n            self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n            self.winner = None\n            self.hashVal = None\n            self.end = None\n\n        def getHash(self):\n            if self.hashVal is None:\n                self.hashVal = 0\n                for i in self.data.reshape(BOARD_ROWS * BOARD_COLS):\n                    if i == -1:\n                        i = 2\n                    self.hashVal = self.hashVal * 3 + i\n            return int(self.hashVal)\n\n        def isEnd(self):\n            if self.end is not None:\n                return self.end\n            results = []\n            for i in range(0, BOARD_ROWS):\n                results.append(np.sum(self.data[i, :]))\n            for i in range(0, BOARD_COLS):\n                results.append(np.sum(self.data[:, i]))\n\n            results.append(0)\n            for i in range(0, BOARD_ROWS):\n                results[-1] += self.data[i, i]\n            results.append(0)\n            for i in range(0, BOARD_ROWS):\n                results[-1] += self.data[i, BOARD_ROWS - 1 - i]\n\n            for result in results:\n                if result == 3:\n                    self.winner = 1\n                    self.end = True\n                    return self.end\n                if result == -3:\n                    self.winner = -1\n                    self.end = True\n                    return self.end\n\n            sum = np.sum(np.abs(self.data))\n            if sum == BOARD_ROWS * BOARD_COLS:\n                self.winner = 0\n                self.end = True\n                return self.end\n\n            self.end = False\n            return self.end\n\n        def nextState(self, i, j, symbol):\n            newState = State()\n            newState.data = np.copy(self.data)\n            newState.data[i, j] = symbol\n            return newState\n\n        # print board\n        def show(self):\n            for i in range(0, BOARD_ROWS):\n                print('-------------')\n                out = '| '\n                for j in range(0, BOARD_COLS):\n                    if self.data[i, j] == 1:\n                        token = '*'\n                    if self.data[i, j] == 0:\n                        token = '0'\n                    if self.data[i, j] == -1:\n                        token = 'x'\n                    out += token + ' | '\n                print(out)\n            print('-------------')\n\n    def getAllStatesImpl(currentState, currentSymbol, allStates):\n        for i in range(0, BOARD_ROWS):\n            for j in range(0, BOARD_COLS):\n                if currentState.data[i][j] == 0:\n                    newState = currentState.nextState(i, j, currentSymbol)\n                    newHash = newState.getHash()\n                    if newHash not in allStates.keys():\n                        isEnd = newState.isEnd()\n                        allStates[newHash] = (newState, isEnd)\n                        if not isEnd:\n                            getAllStatesImpl(newState, -currentSymbol, allStates)\n\n    def getAllStates():\n        currentSymbol = 1\n        currentState = State()\n        allStates = dict()\n        allStates[currentState.getHash()] = (currentState, currentState.isEnd())\n        getAllStatesImpl(currentState, currentSymbol, allStates)\n        return allStates\n\n    allStates = getAllStates()\n\n    class Judger:\n        def __init__(self, player1, player2, feedback=True):\n            self.p1 = player1\n            self.p2 = player2\n            self.feedback = feedback\n            self.currentPlayer = None\n            self.p1Symbol = 1\n            self.p2Symbol = -1\n            self.p1.setSymbol(self.p1Symbol)\n            self.p2.setSymbol(self.p2Symbol)\n            self.currentState = State()\n            self.allStates = allStates\n\n        def giveReward(self):\n            if self.currentState.winner == self.p1Symbol:\n                self.p1.feedReward(1)\n                self.p2.feedReward(0)\n            elif self.currentState.winner == self.p2Symbol:\n                self.p1.feedReward(0)\n                self.p2.feedReward(1)\n            else:\n                self.p1.feedReward(0.1)\n                self.p2.feedReward(0.5)\n\n        def feedCurrentState(self):\n            self.p1.feedState(self.currentState)\n            self.p2.feedState(self.currentState)\n\n        def reset(self):\n            self.p1.reset()\n            self.p2.reset()\n            self.currentState = State()\n            self.currentPlayer = None\n\n        def play(self, show=False):\n            self.reset()\n            self.feedCurrentState()\n            while True:\n                if self.currentPlayer == self.p1:\n                    self.currentPlayer = self.p2\n                else:\n                    self.currentPlayer = self.p1\n                if show:\n                    self.currentState.show()\n                [i, j, symbol] = self.currentPlayer.takeAction()\n                self.currentState = self.currentState.nextState(i, j, symbol)\n                hashValue = self.currentState.getHash()\n                self.currentState, isEnd = self.allStates[hashValue]\n                self.feedCurrentState()\n                if isEnd:\n                    if self.feedback:\n                        self.giveReward()\n                    return self.currentState.winner\n\n    # AI player\n    class Player:\n        def __init__(self, stepSize = 0.1, exploreRate=0.1):\n            self.allStates = allStates\n            self.estimations = dict()\n            self.stepSize = stepSize\n            self.exploreRate = exploreRate\n            self.states = []\n\n        def reset(self):\n            self.states = []\n\n        def setSymbol(self, symbol):\n            self.symbol = symbol\n            for hash in self.allStates.keys():\n                (state, isEnd) = self.allStates[hash]\n                if isEnd:\n                    if state.winner == self.symbol:\n                        self.estimations[hash] = 1.0\n                    else:\n                        self.estimations[hash] = 0\n                else:\n                    self.estimations[hash] = 0.5\n\n        def feedState(self, state):\n            self.states.append(state)\n\n        def feedReward(self, reward):\n            if len(self.states) == 0:\n                return\n            self.states = [state.getHash() for state in self.states]\n            target = reward\n            for latestState in reversed(self.states):\n                value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState])\n                self.estimations[latestState] = value\n                target = value\n            self.states = []\n\n        def takeAction(self):\n            state = self.states[-1]\n            nextStates = []\n            nextPositions = []\n            for i in range(BOARD_ROWS):\n                for j in range(BOARD_COLS):\n                    if state.data[i, j] == 0:\n                        nextPositions.append([i, j])\n                        nextStates.append(state.nextState(i, j, self.symbol).getHash())\n            if np.random.binomial(1, self.exploreRate):\n                np.random.shuffle(nextPositions)\n                self.states = []\n                action = nextPositions[0]\n                action.append(self.symbol)\n                return action\n\n            values = []\n            for hash, pos in zip(nextStates, nextPositions):\n                values.append((self.estimations[hash], pos))\n            np.random.shuffle(values)\n            values.sort(key=lambda x: x[0], reverse=True)\n            action = values[0][1]\n            action.append(self.symbol)\n            return action\n\n        def savePolicy(self):\n            fw = open('optimal_policy_' + str(self.symbol), 'wb')\n            pickle.dump(self.estimations, fw)\n            fw.close()\n\n        def loadPolicy(self):\n            fr = open('optimal_policy_' + str(self.symbol),'rb')\n            self.estimations = pickle.load(fr)\n            fr.close()\n            \n    # | 1 | 2 | 3 |\n    # | 4 | 5 | 6 |\n    # | 7 | 8 | 9 |\n    class HumanPlayer:\n        def __init__(self, stepSize = 0.1, exploreRate=0.1):\n            self.symbol = None\n            self.currentState = None\n            return\n        def reset(self):\n            return\n        def setSymbol(self, symbol):\n            self.symbol = symbol\n            return\n        def feedState(self, state):\n            self.currentState = state\n            return\n        def feedReward(self, reward):\n            return\n        def takeAction(self):\n            data = int(input(\"Input your position:\"))\n            data -= 1\n            i = data // int(BOARD_COLS)\n            j = data % BOARD_COLS\n            if self.currentState.data[i, j] != 0:\n                return self.takeAction()\n            return (i, j, self.symbol)\n\n    def train(epochs=20000):\n        player1 = Player()\n        player2 = Player()\n        judger = Judger(player1, player2)\n        player1Win = 0.0\n        player2Win = 0.0\n        for i in range(0, epochs):\n            print(\"Epoch\", i)\n            winner = judger.play()\n            if winner == 1:\n                player1Win += 1\n            if winner == -1:\n                player2Win += 1\n            judger.reset()\n        print(player1Win / epochs)\n        print(player2Win / epochs)\n        player1.savePolicy()\n        player2.savePolicy()\n\n    def compete(turns=500):\n        player1 = Player(exploreRate=0)\n        player2 = Player(exploreRate=0)\n        judger = Judger(player1, player2, False)\n        player1.loadPolicy()\n        player2.loadPolicy()\n        player1Win = 0.0\n        player2Win = 0.0\n        for i in range(0, turns):\n            print(\"Epoch\", i)\n            winner = judger.play()\n            if winner == 1:\n                player1Win += 1\n            if winner == -1:\n                player2Win += 1\n            judger.reset()\n        print(player1Win / turns)\n        print(player2Win / turns)\n\n    def play():\n        while True:\n            player1 = Player(exploreRate=0)\n            player2 = HumanPlayer()\n            judger = Judger(player1, player2, False)\n            player1.loadPolicy()\n            winner = judger.play(True)\n            if winner == player2.symbol:\n                print(\"Win!\")\n            elif winner == player1.symbol:\n                print(\"Lose!\")\n            else:\n                print(\"Tie!\")\n\n    train()\n    compete()\n    play()\n    \"\"\"\n```\n\n'\\n\\nBOARD_ROWS = 3\\nBOARD_COLS = 3\\nBOARD_SIZE = BOARD_ROWS * BOARD_COLS\\n\\nclass State:\\n    def __init__(self):\\n        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\\n        self.winner = None\\n        self.hashVal = None\\n        self.end = None\\n\\n    def getHash(self):\\n        if self.hashVal is None:\\n            self.hashVal = 0\\n            for i in self.data.reshape(BOARD_ROWS * BOARD_COLS):\\n                if i == -1:\\n                    i = 2\\n                self.hashVal = self.hashVal * 3 + i\\n        return int(self.hashVal)\\n\\n    def isEnd(self):\\n        if self.end is not None:\\n            return self.end\\n        results = []\\n        for i in range(0, BOARD_ROWS):\\n            results.append(np.sum(self.data[i, :]))\\n        for i in range(0, BOARD_COLS):\\n            results.append(np.sum(self.data[:, i]))\\n\\n        results.append(0)\\n        for i in range(0, BOARD_ROWS):\\n            results[-1] += self.data[i, i]\\n        results.append(0)\\n        for i in range(0, BOARD_ROWS):\\n            results[-1] += self.data[i, BOARD_ROWS - 1 - i]\\n\\n        for result in results:\\n            if result == 3:\\n                self.winner = 1\\n                self.end = True\\n                return self.end\\n            if result == -3:\\n                self.winner = -1\\n                self.end = True\\n                return self.end\\n\\n        sum = np.sum(np.abs(self.data))\\n        if sum == BOARD_ROWS * BOARD_COLS:\\n            self.winner = 0\\n            self.end = True\\n            return self.end\\n\\n        self.end = False\\n        return self.end\\n\\n    def nextState(self, i, j, symbol):\\n        newState = State()\\n        newState.data = np.copy(self.data)\\n        newState.data[i, j] = symbol\\n        return newState\\n\\n    # print board\\n    def show(self):\\n        for i in range(0, BOARD_ROWS):\\n            print(\\'-------------\\')\\n            out = \\'| \\'\\n            for j in range(0, BOARD_COLS):\\n                if self.data[i, j] == 1:\\n                    token = \\'*\\'\\n                if self.data[i, j] == 0:\\n                    token = \\'0\\'\\n                if self.data[i, j] == -1:\\n                    token = \\'x\\'\\n                out += token + \\' | \\'\\n            print(out)\\n        print(\\'-------------\\')\\n\\ndef getAllStatesImpl(currentState, currentSymbol, allStates):\\n    for i in range(0, BOARD_ROWS):\\n        for j in range(0, BOARD_COLS):\\n            if currentState.data[i][j] == 0:\\n                newState = currentState.nextState(i, j, currentSymbol)\\n                newHash = newState.getHash()\\n                if newHash not in allStates.keys():\\n                    isEnd = newState.isEnd()\\n                    allStates[newHash] = (newState, isEnd)\\n                    if not isEnd:\\n                        getAllStatesImpl(newState, -currentSymbol, allStates)\\n\\ndef getAllStates():\\n    currentSymbol = 1\\n    currentState = State()\\n    allStates = dict()\\n    allStates[currentState.getHash()] = (currentState, currentState.isEnd())\\n    getAllStatesImpl(currentState, currentSymbol, allStates)\\n    return allStates\\n\\nallStates = getAllStates()\\n\\nclass Judger:\\n    def __init__(self, player1, player2, feedback=True):\\n        self.p1 = player1\\n        self.p2 = player2\\n        self.feedback = feedback\\n        self.currentPlayer = None\\n        self.p1Symbol = 1\\n        self.p2Symbol = -1\\n        self.p1.setSymbol(self.p1Symbol)\\n        self.p2.setSymbol(self.p2Symbol)\\n        self.currentState = State()\\n        self.allStates = allStates\\n\\n    def giveReward(self):\\n        if self.currentState.winner == self.p1Symbol:\\n            self.p1.feedReward(1)\\n            self.p2.feedReward(0)\\n        elif self.currentState.winner == self.p2Symbol:\\n            self.p1.feedReward(0)\\n            self.p2.feedReward(1)\\n        else:\\n            self.p1.feedReward(0.1)\\n            self.p2.feedReward(0.5)\\n\\n    def feedCurrentState(self):\\n        self.p1.feedState(self.currentState)\\n        self.p2.feedState(self.currentState)\\n\\n    def reset(self):\\n        self.p1.reset()\\n        self.p2.reset()\\n        self.currentState = State()\\n        self.currentPlayer = None\\n\\n    def play(self, show=False):\\n        self.reset()\\n        self.feedCurrentState()\\n        while True:\\n            if self.currentPlayer == self.p1:\\n                self.currentPlayer = self.p2\\n            else:\\n                self.currentPlayer = self.p1\\n            if show:\\n                self.currentState.show()\\n            [i, j, symbol] = self.currentPlayer.takeAction()\\n            self.currentState = self.currentState.nextState(i, j, symbol)\\n            hashValue = self.currentState.getHash()\\n            self.currentState, isEnd = self.allStates[hashValue]\\n            self.feedCurrentState()\\n            if isEnd:\\n                if self.feedback:\\n                    self.giveReward()\\n                return self.currentState.winner\\n\\n# AI player\\nclass Player:\\n    def __init__(self, stepSize = 0.1, exploreRate=0.1):\\n        self.allStates = allStates\\n        self.estimations = dict()\\n        self.stepSize = stepSize\\n        self.exploreRate = exploreRate\\n        self.states = []\\n\\n    def reset(self):\\n        self.states = []\\n\\n    def setSymbol(self, symbol):\\n        self.symbol = symbol\\n        for hash in self.allStates.keys():\\n            (state, isEnd) = self.allStates[hash]\\n            if isEnd:\\n                if state.winner == self.symbol:\\n                    self.estimations[hash] = 1.0\\n                else:\\n                    self.estimations[hash] = 0\\n            else:\\n                self.estimations[hash] = 0.5\\n\\n    def feedState(self, state):\\n        self.states.append(state)\\n\\n    def feedReward(self, reward):\\n        if len(self.states) == 0:\\n            return\\n        self.states = [state.getHash() for state in self.states]\\n        target = reward\\n        for latestState in reversed(self.states):\\n            value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState])\\n            self.estimations[latestState] = value\\n            target = value\\n        self.states = []\\n\\n    def takeAction(self):\\n        state = self.states[-1]\\n        nextStates = []\\n        nextPositions = []\\n        for i in range(BOARD_ROWS):\\n            for j in range(BOARD_COLS):\\n                if state.data[i, j] == 0:\\n                    nextPositions.append([i, j])\\n                    nextStates.append(state.nextState(i, j, self.symbol).getHash())\\n        if np.random.binomial(1, self.exploreRate):\\n            np.random.shuffle(nextPositions)\\n            self.states = []\\n            action = nextPositions[0]\\n            action.append(self.symbol)\\n            return action\\n\\n        values = []\\n        for hash, pos in zip(nextStates, nextPositions):\\n            values.append((self.estimations[hash], pos))\\n        np.random.shuffle(values)\\n        values.sort(key=lambda x: x[0], reverse=True)\\n        action = values[0][1]\\n        action.append(self.symbol)\\n        return action\\n\\n    def savePolicy(self):\\n        fw = open(\\'optimal_policy_\\' + str(self.symbol), \\'wb\\')\\n        pickle.dump(self.estimations, fw)\\n        fw.close()\\n\\n    def loadPolicy(self):\\n        fr = open(\\'optimal_policy_\\' + str(self.symbol),\\'rb\\')\\n        self.estimations = pickle.load(fr)\\n        fr.close()\\n        \\n# | 1 | 2 | 3 |\\n# | 4 | 5 | 6 |\\n# | 7 | 8 | 9 |\\nclass HumanPlayer:\\n    def __init__(self, stepSize = 0.1, exploreRate=0.1):\\n        self.symbol = None\\n        self.currentState = None\\n        return\\n    def reset(self):\\n        return\\n    def setSymbol(self, symbol):\\n        self.symbol = symbol\\n        return\\n    def feedState(self, state):\\n        self.currentState = state\\n        return\\n    def feedReward(self, reward):\\n        return\\n    def takeAction(self):\\n        data = int(input(\"Input your position:\"))\\n        data -= 1\\n        i = data // int(BOARD_COLS)\\n        j = data % BOARD_COLS\\n        if self.currentState.data[i, j] != 0:\\n            return self.takeAction()\\n        return (i, j, self.symbol)\\n\\ndef train(epochs=20000):\\n    player1 = Player()\\n    player2 = Player()\\n    judger = Judger(player1, player2)\\n    player1Win = 0.0\\n    player2Win = 0.0\\n    for i in range(0, epochs):\\n        print(\"Epoch\", i)\\n        winner = judger.play()\\n        if winner == 1:\\n            player1Win += 1\\n        if winner == -1:\\n            player2Win += 1\\n        judger.reset()\\n    print(player1Win / epochs)\\n    print(player2Win / epochs)\\n    player1.savePolicy()\\n    player2.savePolicy()\\n\\ndef compete(turns=500):\\n    player1 = Player(exploreRate=0)\\n    player2 = Player(exploreRate=0)\\n    judger = Judger(player1, player2, False)\\n    player1.loadPolicy()\\n    player2.loadPolicy()\\n    player1Win = 0.0\\n    player2Win = 0.0\\n    for i in range(0, turns):\\n        print(\"Epoch\", i)\\n        winner = judger.play()\\n        if winner == 1:\\n            player1Win += 1\\n        if winner == -1:\\n            player2Win += 1\\n        judger.reset()\\n    print(player1Win / turns)\\n    print(player2Win / turns)\\n\\ndef play():\\n    while True:\\n        player1 = Player(exploreRate=0)\\n        player2 = HumanPlayer()\\n        judger = Judger(player1, player2, False)\\n        player1.loadPolicy()\\n        winner = judger.play(True)\\n        if winner == player2.symbol:\\n            print(\"Win!\")\\n        elif winner == player1.symbol:\\n            print(\"Lose!\")\\n        else:\\n            print(\"Tie!\")\\n\\ntrain()\\ncompete()\\nplay()\\n'"
  }
]