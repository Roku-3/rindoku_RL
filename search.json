[
  {
    "objectID": "index.html#輪読会にあたって",
    "href": "index.html#輪読会にあたって",
    "title": "強化学習 輪読会",
    "section": "輪読会にあたって",
    "text": "輪読会にあたって\n第１章以降、とくに第Ⅱ部（第９章〜）からはそれぞれが独立した内容が多い。\n最初にすべての難易度を把握するのは難しいので、毎週次回の担当範囲を決めることとする。\n本の内容に沿って解説していき、わからないところがあれば聞き手はよこやりを入れて質問をしてよい。\n担当者はすべての質問に答えられるように準備する。"
  },
  {
    "objectID": "index.html#当書で使われる記号",
    "href": "index.html#当書で使われる記号",
    "title": "強化学習 輪読会",
    "section": "当書で使われる記号",
    "text": "当書で使われる記号\n\n```{python}\n1 + 1\n```\n\n2"
  },
  {
    "objectID": "index.html#今後の進め方",
    "href": "index.html#今後の進め方",
    "title": "強化学習 輪読会",
    "section": "今後の進め方",
    "text": "今後の進め方\n二人で共通の文書を編集する形式に賛成であれば、次回までにこの文書をサーバーにアップロードし、峻平への導入方法を考えておく。\n\n\n\n日時\nろく\n峻平\n\n\n\n\n11/30\n序章\n\n\n\n12/5\n第一章\n\n\n\n12/12\n第二章前半？\n第二章後半？\n\n\n12/19\n\n\n\n\n12/26\n\n\n\n\n1/2\n\n\n\n\n\nそうでなければ、例えば一人はbook形式、もう一人はスライドを用いて発表する。"
  },
  {
    "objectID": "index.html#文書の編集方法",
    "href": "index.html#文書の編集方法",
    "title": "強化学習 輪読会",
    "section": "文書の編集方法",
    "text": "文書の編集方法\n\ngit環境を整える。（wslがおすすめ）\nrstudioとquartoをインストール\nhttps://github.com/Roku-3/rindoku_RLにアクセスし、mainブランチでリポジトリをクローンする。rstudioで開く。\n\n\n編集内容を適応する\ngitの基本コマンドは以下の通り。\ngit pull origin HEAD            # githubからローカルに差分をもってくる\n\ngit add .                       # 全てのファイルをコミット対象にする\n\ngit commit -m \"edit chapter 2\"  # 編集内容を表すコメント付きでコミットする。\n                                # -mはコミットメッセージを一行にするという意味\n                                \ngit push origin HEAD            # githubに情報を送る\n2人が同じ場所を変更していた場合、コンフリクトが発生する。その時は必ず解消してからpushを行う。\ngit branchと入力したときにmainになっていることを確認してからpushすること。\npushが正常に行われると自動でデプロイ（サイトが公開）される。\n\n\n今後の運用\n一人が文書を変更して適応した後、細かいミスなどで再編集するのは手間。また間違ってファイルを削除してしまった時の危険性も高い。\nそのため、後々はプルリクエストを出してもう一人が確認する形式にしようと思う。\nプルリクエストは手軽に人の編集を確認・訂正できるgithubの機能である。"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "序章のまとめ",
    "section": "",
    "text": "第１版序文\n強化学習は197X年ごろから存在が知られてきた。\nしかし、筆者は環境からどのようにして学習するか、ということにさほど注目されていないことに気がついた。"
  },
  {
    "objectID": "chapter1.html#具体的な構成要素",
    "href": "chapter1.html#具体的な構成要素",
    "title": "第１章:強化学習とは",
    "section": "具体的な構成要素",
    "text": "具体的な構成要素\n\n\n\n\n\n\n\n方策(policy)\nある状態でのそこからとるべき行動への写像\n\n\n報酬(reward)\n各時間ステップごとの行動を評価して環境からエージェントに送られるもの。\n\n\n価値関数(value function)\n報酬とは違い、その状態の長期的な価値を導く。\n\n\nモデル(model)\n環境の挙動を模倣する、主に未来の環境（状態遷移と報酬）を予測するための関数群。\n\n\n\n強化学習の最終目標はこの報酬を最大化することである。\n４つ目の環境モデルは必須要素ではなく、使用したものをモデルベース、していないものをモデルフリー手法と呼ぶ。"
  },
  {
    "objectID": "chapter1.html#進化的手法との違い",
    "href": "chapter1.html#進化的手法との違い",
    "title": "第１章:強化学習とは",
    "section": "進化的手法との違い",
    "text": "進化的手法との違い\n（当書では別のものとして捉えているが、進化的手法を強化学習アルゴリズムの一派とする考え方もある。）\n遺伝的アルゴリズム、遺伝的プログラミング、焼きなまし法などを進化的手法と呼ぶ。これらは生物のように優れたエージェントの特徴を引き継いで次の世代が獲得する報酬を大きくする。\n環境からの情報をもとに行動し、報酬を最大化するという点で強化学習と似ているが、最初に述べた環境との相互作用がないという点で異なる。\nつまり、進化的手法ではどの状態でどの行動を取ったのか、という情報を使わない。エージェントが環境の状態を知覚しづらい（数値化が難しい）場合などを除いて、強化学習のほうが効率的に学習できることが多い。"
  },
  {
    "objectID": "chapter1.html#シンプルな例三目並べ",
    "href": "chapter1.html#シンプルな例三目並べ",
    "title": "第１章:強化学習とは",
    "section": "シンプルな例：三目並べ",
    "text": "シンプルな例：三目並べ\n強化学習を用いて三目並べに勝つことを目標とするエージェントを作る。\n以下のような行動木を作成する。それぞれの状態（盤面）には0~1の価値が設定されている。どうやっても負ける状態には0、既に勝っている状態には1、それ以外には0.5が初期値として割り当てられる。\n学習中、基本的には価値が大きい、勝率が高い手を選ぶが、すべての状態を試すためにランダム性を持った行動選択（探索的な手）をさせる。\n状態の価値の更新は以下の式で行う。\n\\[\nV(s_t) \\leftarrow V(s_t)+\\alpha\\left[V\\left(s_{t+1}\\right)-V(s_t)\\right]\n\\]\n\\(V(s)\\)は状態\\(s\\)の推定価値を表し、次の手を選択した際、次の状態の価値に近づけている。ステップサイズパラメータ\\(\\alpha\\)によって学習率を調整している。\nこの更新則はTD学習（時間差分学習; temporal-difference learning）と呼ばれる。"
  },
  {
    "objectID": "chapter1.html#練習問題",
    "href": "chapter1.html#練習問題",
    "title": "第１章:強化学習とは",
    "section": "練習問題",
    "text": "練習問題\n1.1 自己対戦（self play）\n\n```{python}\n    '''\n    import numpy as np\n    import pickle\n    '''\n```\n\n'\\nimport numpy as np\\nimport pickle\\n'\n\n\n\n```{python}\n    \"\"\"\n\n    BOARD_ROWS = 3\n    BOARD_COLS = 3\n    BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n\n    class State:\n        def __init__(self):\n            self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n            self.winner = None\n            self.hashVal = None\n            self.end = None\n\n        def getHash(self):\n            if self.hashVal is None:\n                self.hashVal = 0\n                for i in self.data.reshape(BOARD_ROWS * BOARD_COLS):\n                    if i == -1:\n                        i = 2\n                    self.hashVal = self.hashVal * 3 + i\n            return int(self.hashVal)\n\n        def isEnd(self):\n            if self.end is not None:\n                return self.end\n            results = []\n            for i in range(0, BOARD_ROWS):\n                results.append(np.sum(self.data[i, :]))\n            for i in range(0, BOARD_COLS):\n                results.append(np.sum(self.data[:, i]))\n\n            results.append(0)\n            for i in range(0, BOARD_ROWS):\n                results[-1] += self.data[i, i]\n            results.append(0)\n            for i in range(0, BOARD_ROWS):\n                results[-1] += self.data[i, BOARD_ROWS - 1 - i]\n\n            for result in results:\n                if result == 3:\n                    self.winner = 1\n                    self.end = True\n                    return self.end\n                if result == -3:\n                    self.winner = -1\n                    self.end = True\n                    return self.end\n\n            sum = np.sum(np.abs(self.data))\n            if sum == BOARD_ROWS * BOARD_COLS:\n                self.winner = 0\n                self.end = True\n                return self.end\n\n            self.end = False\n            return self.end\n\n        def nextState(self, i, j, symbol):\n            newState = State()\n            newState.data = np.copy(self.data)\n            newState.data[i, j] = symbol\n            return newState\n\n        # print board\n        def show(self):\n            for i in range(0, BOARD_ROWS):\n                print('-------------')\n                out = '| '\n                for j in range(0, BOARD_COLS):\n                    if self.data[i, j] == 1:\n                        token = '*'\n                    if self.data[i, j] == 0:\n                        token = '0'\n                    if self.data[i, j] == -1:\n                        token = 'x'\n                    out += token + ' | '\n                print(out)\n            print('-------------')\n\n    def getAllStatesImpl(currentState, currentSymbol, allStates):\n        for i in range(0, BOARD_ROWS):\n            for j in range(0, BOARD_COLS):\n                if currentState.data[i][j] == 0:\n                    newState = currentState.nextState(i, j, currentSymbol)\n                    newHash = newState.getHash()\n                    if newHash not in allStates.keys():\n                        isEnd = newState.isEnd()\n                        allStates[newHash] = (newState, isEnd)\n                        if not isEnd:\n                            getAllStatesImpl(newState, -currentSymbol, allStates)\n\n    def getAllStates():\n        currentSymbol = 1\n        currentState = State()\n        allStates = dict()\n        allStates[currentState.getHash()] = (currentState, currentState.isEnd())\n        getAllStatesImpl(currentState, currentSymbol, allStates)\n        return allStates\n\n    allStates = getAllStates()\n\n    class Judger:\n        def __init__(self, player1, player2, feedback=True):\n            self.p1 = player1\n            self.p2 = player2\n            self.feedback = feedback\n            self.currentPlayer = None\n            self.p1Symbol = 1\n            self.p2Symbol = -1\n            self.p1.setSymbol(self.p1Symbol)\n            self.p2.setSymbol(self.p2Symbol)\n            self.currentState = State()\n            self.allStates = allStates\n\n        def giveReward(self):\n            if self.currentState.winner == self.p1Symbol:\n                self.p1.feedReward(1)\n                self.p2.feedReward(0)\n            elif self.currentState.winner == self.p2Symbol:\n                self.p1.feedReward(0)\n                self.p2.feedReward(1)\n            else:\n                self.p1.feedReward(0.1)\n                self.p2.feedReward(0.5)\n\n        def feedCurrentState(self):\n            self.p1.feedState(self.currentState)\n            self.p2.feedState(self.currentState)\n\n        def reset(self):\n            self.p1.reset()\n            self.p2.reset()\n            self.currentState = State()\n            self.currentPlayer = None\n\n        def play(self, show=False):\n            self.reset()\n            self.feedCurrentState()\n            while True:\n                if self.currentPlayer == self.p1:\n                    self.currentPlayer = self.p2\n                else:\n                    self.currentPlayer = self.p1\n                if show:\n                    self.currentState.show()\n                [i, j, symbol] = self.currentPlayer.takeAction()\n                self.currentState = self.currentState.nextState(i, j, symbol)\n                hashValue = self.currentState.getHash()\n                self.currentState, isEnd = self.allStates[hashValue]\n                self.feedCurrentState()\n                if isEnd:\n                    if self.feedback:\n                        self.giveReward()\n                    return self.currentState.winner\n\n    # AI player\n    class Player:\n        def __init__(self, stepSize = 0.1, exploreRate=0.1):\n            self.allStates = allStates\n            self.estimations = dict()\n            self.stepSize = stepSize\n            self.exploreRate = exploreRate\n            self.states = []\n\n        def reset(self):\n            self.states = []\n\n        def setSymbol(self, symbol):\n            self.symbol = symbol\n            for hash in self.allStates.keys():\n                (state, isEnd) = self.allStates[hash]\n                if isEnd:\n                    if state.winner == self.symbol:\n                        self.estimations[hash] = 1.0\n                    else:\n                        self.estimations[hash] = 0\n                else:\n                    self.estimations[hash] = 0.5\n\n        def feedState(self, state):\n            self.states.append(state)\n\n        def feedReward(self, reward):\n            if len(self.states) == 0:\n                return\n            self.states = [state.getHash() for state in self.states]\n            target = reward\n            for latestState in reversed(self.states):\n                value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState])\n                self.estimations[latestState] = value\n                target = value\n            self.states = []\n\n        def takeAction(self):\n            state = self.states[-1]\n            nextStates = []\n            nextPositions = []\n            for i in range(BOARD_ROWS):\n                for j in range(BOARD_COLS):\n                    if state.data[i, j] == 0:\n                        nextPositions.append([i, j])\n                        nextStates.append(state.nextState(i, j, self.symbol).getHash())\n            if np.random.binomial(1, self.exploreRate):\n                np.random.shuffle(nextPositions)\n                self.states = []\n                action = nextPositions[0]\n                action.append(self.symbol)\n                return action\n\n            values = []\n            for hash, pos in zip(nextStates, nextPositions):\n                values.append((self.estimations[hash], pos))\n            np.random.shuffle(values)\n            values.sort(key=lambda x: x[0], reverse=True)\n            action = values[0][1]\n            action.append(self.symbol)\n            return action\n\n        def savePolicy(self):\n            fw = open('optimal_policy_' + str(self.symbol), 'wb')\n            pickle.dump(self.estimations, fw)\n            fw.close()\n\n        def loadPolicy(self):\n            fr = open('optimal_policy_' + str(self.symbol),'rb')\n            self.estimations = pickle.load(fr)\n            fr.close()\n            \n    # | 1 | 2 | 3 |\n    # | 4 | 5 | 6 |\n    # | 7 | 8 | 9 |\n    class HumanPlayer:\n        def __init__(self, stepSize = 0.1, exploreRate=0.1):\n            self.symbol = None\n            self.currentState = None\n            return\n        def reset(self):\n            return\n        def setSymbol(self, symbol):\n            self.symbol = symbol\n            return\n        def feedState(self, state):\n            self.currentState = state\n            return\n        def feedReward(self, reward):\n            return\n        def takeAction(self):\n            data = int(input(\"Input your position:\"))\n            data -= 1\n            i = data // int(BOARD_COLS)\n            j = data % BOARD_COLS\n            if self.currentState.data[i, j] != 0:\n                return self.takeAction()\n            return (i, j, self.symbol)\n\n    def train(epochs=20000):\n        player1 = Player()\n        player2 = Player()\n        judger = Judger(player1, player2)\n        player1Win = 0.0\n        player2Win = 0.0\n        for i in range(0, epochs):\n            print(\"Epoch\", i)\n            winner = judger.play()\n            if winner == 1:\n                player1Win += 1\n            if winner == -1:\n                player2Win += 1\n            judger.reset()\n        print(player1Win / epochs)\n        print(player2Win / epochs)\n        player1.savePolicy()\n        player2.savePolicy()\n\n    def compete(turns=500):\n        player1 = Player(exploreRate=0)\n        player2 = Player(exploreRate=0)\n        judger = Judger(player1, player2, False)\n        player1.loadPolicy()\n        player2.loadPolicy()\n        player1Win = 0.0\n        player2Win = 0.0\n        for i in range(0, turns):\n            print(\"Epoch\", i)\n            winner = judger.play()\n            if winner == 1:\n                player1Win += 1\n            if winner == -1:\n                player2Win += 1\n            judger.reset()\n        print(player1Win / turns)\n        print(player2Win / turns)\n\n    def play():\n        while True:\n            player1 = Player(exploreRate=0)\n            player2 = HumanPlayer()\n            judger = Judger(player1, player2, False)\n            player1.loadPolicy()\n            winner = judger.play(True)\n            if winner == player2.symbol:\n                print(\"Win!\")\n            elif winner == player1.symbol:\n                print(\"Lose!\")\n            else:\n                print(\"Tie!\")\n\n    train()\n    compete()\n    play()\n    \"\"\"\n```\n\n'\\n\\nBOARD_ROWS = 3\\nBOARD_COLS = 3\\nBOARD_SIZE = BOARD_ROWS * BOARD_COLS\\n\\nclass State:\\n    def __init__(self):\\n        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\\n        self.winner = None\\n        self.hashVal = None\\n        self.end = None\\n\\n    def getHash(self):\\n        if self.hashVal is None:\\n            self.hashVal = 0\\n            for i in self.data.reshape(BOARD_ROWS * BOARD_COLS):\\n                if i == -1:\\n                    i = 2\\n                self.hashVal = self.hashVal * 3 + i\\n        return int(self.hashVal)\\n\\n    def isEnd(self):\\n        if self.end is not None:\\n            return self.end\\n        results = []\\n        for i in range(0, BOARD_ROWS):\\n            results.append(np.sum(self.data[i, :]))\\n        for i in range(0, BOARD_COLS):\\n            results.append(np.sum(self.data[:, i]))\\n\\n        results.append(0)\\n        for i in range(0, BOARD_ROWS):\\n            results[-1] += self.data[i, i]\\n        results.append(0)\\n        for i in range(0, BOARD_ROWS):\\n            results[-1] += self.data[i, BOARD_ROWS - 1 - i]\\n\\n        for result in results:\\n            if result == 3:\\n                self.winner = 1\\n                self.end = True\\n                return self.end\\n            if result == -3:\\n                self.winner = -1\\n                self.end = True\\n                return self.end\\n\\n        sum = np.sum(np.abs(self.data))\\n        if sum == BOARD_ROWS * BOARD_COLS:\\n            self.winner = 0\\n            self.end = True\\n            return self.end\\n\\n        self.end = False\\n        return self.end\\n\\n    def nextState(self, i, j, symbol):\\n        newState = State()\\n        newState.data = np.copy(self.data)\\n        newState.data[i, j] = symbol\\n        return newState\\n\\n    # print board\\n    def show(self):\\n        for i in range(0, BOARD_ROWS):\\n            print(\\'-------------\\')\\n            out = \\'| \\'\\n            for j in range(0, BOARD_COLS):\\n                if self.data[i, j] == 1:\\n                    token = \\'*\\'\\n                if self.data[i, j] == 0:\\n                    token = \\'0\\'\\n                if self.data[i, j] == -1:\\n                    token = \\'x\\'\\n                out += token + \\' | \\'\\n            print(out)\\n        print(\\'-------------\\')\\n\\ndef getAllStatesImpl(currentState, currentSymbol, allStates):\\n    for i in range(0, BOARD_ROWS):\\n        for j in range(0, BOARD_COLS):\\n            if currentState.data[i][j] == 0:\\n                newState = currentState.nextState(i, j, currentSymbol)\\n                newHash = newState.getHash()\\n                if newHash not in allStates.keys():\\n                    isEnd = newState.isEnd()\\n                    allStates[newHash] = (newState, isEnd)\\n                    if not isEnd:\\n                        getAllStatesImpl(newState, -currentSymbol, allStates)\\n\\ndef getAllStates():\\n    currentSymbol = 1\\n    currentState = State()\\n    allStates = dict()\\n    allStates[currentState.getHash()] = (currentState, currentState.isEnd())\\n    getAllStatesImpl(currentState, currentSymbol, allStates)\\n    return allStates\\n\\nallStates = getAllStates()\\n\\nclass Judger:\\n    def __init__(self, player1, player2, feedback=True):\\n        self.p1 = player1\\n        self.p2 = player2\\n        self.feedback = feedback\\n        self.currentPlayer = None\\n        self.p1Symbol = 1\\n        self.p2Symbol = -1\\n        self.p1.setSymbol(self.p1Symbol)\\n        self.p2.setSymbol(self.p2Symbol)\\n        self.currentState = State()\\n        self.allStates = allStates\\n\\n    def giveReward(self):\\n        if self.currentState.winner == self.p1Symbol:\\n            self.p1.feedReward(1)\\n            self.p2.feedReward(0)\\n        elif self.currentState.winner == self.p2Symbol:\\n            self.p1.feedReward(0)\\n            self.p2.feedReward(1)\\n        else:\\n            self.p1.feedReward(0.1)\\n            self.p2.feedReward(0.5)\\n\\n    def feedCurrentState(self):\\n        self.p1.feedState(self.currentState)\\n        self.p2.feedState(self.currentState)\\n\\n    def reset(self):\\n        self.p1.reset()\\n        self.p2.reset()\\n        self.currentState = State()\\n        self.currentPlayer = None\\n\\n    def play(self, show=False):\\n        self.reset()\\n        self.feedCurrentState()\\n        while True:\\n            if self.currentPlayer == self.p1:\\n                self.currentPlayer = self.p2\\n            else:\\n                self.currentPlayer = self.p1\\n            if show:\\n                self.currentState.show()\\n            [i, j, symbol] = self.currentPlayer.takeAction()\\n            self.currentState = self.currentState.nextState(i, j, symbol)\\n            hashValue = self.currentState.getHash()\\n            self.currentState, isEnd = self.allStates[hashValue]\\n            self.feedCurrentState()\\n            if isEnd:\\n                if self.feedback:\\n                    self.giveReward()\\n                return self.currentState.winner\\n\\n# AI player\\nclass Player:\\n    def __init__(self, stepSize = 0.1, exploreRate=0.1):\\n        self.allStates = allStates\\n        self.estimations = dict()\\n        self.stepSize = stepSize\\n        self.exploreRate = exploreRate\\n        self.states = []\\n\\n    def reset(self):\\n        self.states = []\\n\\n    def setSymbol(self, symbol):\\n        self.symbol = symbol\\n        for hash in self.allStates.keys():\\n            (state, isEnd) = self.allStates[hash]\\n            if isEnd:\\n                if state.winner == self.symbol:\\n                    self.estimations[hash] = 1.0\\n                else:\\n                    self.estimations[hash] = 0\\n            else:\\n                self.estimations[hash] = 0.5\\n\\n    def feedState(self, state):\\n        self.states.append(state)\\n\\n    def feedReward(self, reward):\\n        if len(self.states) == 0:\\n            return\\n        self.states = [state.getHash() for state in self.states]\\n        target = reward\\n        for latestState in reversed(self.states):\\n            value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState])\\n            self.estimations[latestState] = value\\n            target = value\\n        self.states = []\\n\\n    def takeAction(self):\\n        state = self.states[-1]\\n        nextStates = []\\n        nextPositions = []\\n        for i in range(BOARD_ROWS):\\n            for j in range(BOARD_COLS):\\n                if state.data[i, j] == 0:\\n                    nextPositions.append([i, j])\\n                    nextStates.append(state.nextState(i, j, self.symbol).getHash())\\n        if np.random.binomial(1, self.exploreRate):\\n            np.random.shuffle(nextPositions)\\n            self.states = []\\n            action = nextPositions[0]\\n            action.append(self.symbol)\\n            return action\\n\\n        values = []\\n        for hash, pos in zip(nextStates, nextPositions):\\n            values.append((self.estimations[hash], pos))\\n        np.random.shuffle(values)\\n        values.sort(key=lambda x: x[0], reverse=True)\\n        action = values[0][1]\\n        action.append(self.symbol)\\n        return action\\n\\n    def savePolicy(self):\\n        fw = open(\\'optimal_policy_\\' + str(self.symbol), \\'wb\\')\\n        pickle.dump(self.estimations, fw)\\n        fw.close()\\n\\n    def loadPolicy(self):\\n        fr = open(\\'optimal_policy_\\' + str(self.symbol),\\'rb\\')\\n        self.estimations = pickle.load(fr)\\n        fr.close()\\n        \\n# | 1 | 2 | 3 |\\n# | 4 | 5 | 6 |\\n# | 7 | 8 | 9 |\\nclass HumanPlayer:\\n    def __init__(self, stepSize = 0.1, exploreRate=0.1):\\n        self.symbol = None\\n        self.currentState = None\\n        return\\n    def reset(self):\\n        return\\n    def setSymbol(self, symbol):\\n        self.symbol = symbol\\n        return\\n    def feedState(self, state):\\n        self.currentState = state\\n        return\\n    def feedReward(self, reward):\\n        return\\n    def takeAction(self):\\n        data = int(input(\"Input your position:\"))\\n        data -= 1\\n        i = data // int(BOARD_COLS)\\n        j = data % BOARD_COLS\\n        if self.currentState.data[i, j] != 0:\\n            return self.takeAction()\\n        return (i, j, self.symbol)\\n\\ndef train(epochs=20000):\\n    player1 = Player()\\n    player2 = Player()\\n    judger = Judger(player1, player2)\\n    player1Win = 0.0\\n    player2Win = 0.0\\n    for i in range(0, epochs):\\n        print(\"Epoch\", i)\\n        winner = judger.play()\\n        if winner == 1:\\n            player1Win += 1\\n        if winner == -1:\\n            player2Win += 1\\n        judger.reset()\\n    print(player1Win / epochs)\\n    print(player2Win / epochs)\\n    player1.savePolicy()\\n    player2.savePolicy()\\n\\ndef compete(turns=500):\\n    player1 = Player(exploreRate=0)\\n    player2 = Player(exploreRate=0)\\n    judger = Judger(player1, player2, False)\\n    player1.loadPolicy()\\n    player2.loadPolicy()\\n    player1Win = 0.0\\n    player2Win = 0.0\\n    for i in range(0, turns):\\n        print(\"Epoch\", i)\\n        winner = judger.play()\\n        if winner == 1:\\n            player1Win += 1\\n        if winner == -1:\\n            player2Win += 1\\n        judger.reset()\\n    print(player1Win / turns)\\n    print(player2Win / turns)\\n\\ndef play():\\n    while True:\\n        player1 = Player(exploreRate=0)\\n        player2 = HumanPlayer()\\n        judger = Judger(player1, player2, False)\\n        player1.loadPolicy()\\n        winner = judger.play(True)\\n        if winner == player2.symbol:\\n            print(\"Win!\")\\n        elif winner == player1.symbol:\\n            print(\"Lose!\")\\n        else:\\n            print(\"Tie!\")\\n\\ntrain()\\ncompete()\\nplay()\\n'"
  },
  {
    "objectID": "chapter2.html#k本腕バンディット問題",
    "href": "chapter2.html#k本腕バンディット問題",
    "title": "第２章多腕バンディット問題",
    "section": "k本腕バンディット問題",
    "text": "k本腕バンディット問題\n\\(k\\)個の選択肢があり、どれか1つを選ぶと選択に依存した定常確率分布から報酬が発生する。これを繰り返し、ある決められた時間ステップにおいて期待合計報酬を最大化する問題を\\(k\\)本腕バンディット問題という。以後、時間ステップtで選択された行動を\\(A_t\\)、対応する報酬を\\(R_t\\)とする。\\(a\\)が選択された時の確率変数を\\(X_a\\)とすると、\\(a\\)が選択されたときの期待報酬\\(q_*(a)\\)を次のように定義する。\n\\[\nq_*(a):=𝔼[X_a]\n\\]\n時間ステップtの行動aの価値の推定値を\\(Q_t(a)\\)とする。これを\\(q_*(a)\\)に近づけたい。 推定価値が最大となる行動をグリーディ行動といい、知識を活用しているという。そうでないときは探索しているという。"
  },
  {
    "objectID": "chapter2.html#行動価値手法",
    "href": "chapter2.html#行動価値手法",
    "title": "第２章多腕バンディット問題",
    "section": "行動価値手法",
    "text": "行動価値手法\n行動価値の推定をする方法の総称を行動価値手法という。行動価値を推定する自然な方法として、実際に得られた報酬を平均することが挙げられる。\n\\[\nQ_t(a):= \\frac{tより前にaを行ったときの報酬の合計}{tの前までにaを行った回数}=\\frac{\\sum_{i=1}^{t-1}R_i･𝟙_{A_i=a}}{\\sum_{i=1}^{t-1}𝟙_{A_i=a}}\n\\]\nただし\\(𝟙_{条件}\\)は条件が真のとき1、偽のとき0をとるものとする。これを行動価値推定のためのサンプル平均法という。毎回最も推定価値が高い行動をするグリーディ行動選択法は次式で表される。\n\\[\nA_t=\\text{arg max}_aQ_t(a)\n\\]\nただし\\(\\text{arg max}_a\\)は\\(Q_t(a)\\)が最大となるような\\(a\\)をかえす。これをベースとして、探索を行わせるために毎回確率\\(\\epsilon\\)でランダムな選択を行わせる手法を\\(\\epsilon\\)-グリーディ法という。"
  },
  {
    "objectID": "chapter2.html#本腕バンディットによる実験",
    "href": "chapter2.html#本腕バンディットによる実験",
    "title": "第２章多腕バンディット問題",
    "section": "10本腕バンディットによる実験",
    "text": "10本腕バンディットによる実験\n2000個のk本腕バンディット問題を、k=10とし、次のようにランダムに生成する：各\\(q_*(a)\\)は標準正規分布(\\(N(0,1)\\))に従って生成し、各\\(X_a\\)は正規分布\\(N(q_*(a),1)\\)に従うとする。各初期推定値\\(Q_1(a)\\)は0とする。各問題で、1000ステップの実行を1回の試行とする。\n2000個の問題それぞれに対しグリーディ法と二つの\\(\\epsilon\\)-グリーディ法を試行し、2000回繰り返し平均をとった結果は以下のようになる。\n\n図から\\(\\epsilon=0.1\\)のときは\\(\\epsilon=0.01\\)のときよりも素早く最適行動をとれていることが分かるが、超長期的な時間ステップでみれば最終的には\\(\\epsilon=0.01\\)のときの方が良い結果を示すだろうということが分かる。"
  },
  {
    "objectID": "chapter2.html#逐次的実装",
    "href": "chapter2.html#逐次的実装",
    "title": "第２章多腕バンディット問題",
    "section": "逐次的実装",
    "text": "逐次的実装\nサンプル平均の計算がどのように簡略化できるのかを紹介する。簡単のため、1本腕バンディット問題でのサンプル平均を考える。ステップ\\(n\\)時の推定報酬\\(Q_n\\)は次式で表される。\n\\[\nQ_n:=\\frac{R_1+R_2+･･･+R_n}{n-1}\n\\]\n各\\(R_n\\)を記録して毎回\\(Q_n\\)を求めるやり方は必要メモリと計算量が増大してしまうが、実際は次式のように逐一更新できる。\n\\[\nQ_{n+1}=\\frac{1}{n}\\sum_{i=1}^{n}R_i\\\\ =\\frac{1}{n}(R_n+\\sum_{i=1}^{n-1}R_i)\\\\ =\\frac{1}{n}(R_n+(n-1)Q_n)\\\\ =Q_n+\\frac{1}{n}(R_n-Q_n)\n\\]\nこうすればメモリは\\(Q_n\\)と\\(n\\)の分だけで良く、計算量も最後の式のみで良くなる。これの一般形は次式で表される。\n\\[\nNewEstimate \\leftarrow OldEstimate + StepSize\\ [Target - OldEstimate]\n\\]\n\\([Target - OldEstimate]\\)は推定の誤差を表している。ステップが進むにつれ推定は\\(Target\\)(今回は\\(n\\)番目の報酬)に近づいていく。今回では\\(StepSize\\)は\\(1/n\\)と一定だが、ステップに応じて変動することもある。以後ステップサイズパラメータを\\(\\alpha\\)や\\(\\alpha_t(a)\\)と表す。\n逐次的計算によるサンプル平均と\\(\\epsilon\\)-行動選択を使ったk本腕バンディットアルゴリズムの疑似コードは以下のようになる。"
  },
  {
    "objectID": "chapter2.html#非定常問題を調べる",
    "href": "chapter2.html#非定常問題を調べる",
    "title": "第２章多腕バンディット問題",
    "section": "非定常問題を調べる",
    "text": "非定常問題を調べる\n非定常的な問題、すなわち、行動に対する報酬(つまり\\(X_a\\))が時間ステップごとに変化する問題の場合については,、新しい報酬の方に重みをつける方が理にかなっている。そこで、逐次更新則は定数\\(\\alpha \\in (0,1]\\)を用いて次のように修正される。\n\\[\nQ_{n+1}:=Q_n+\\alpha[R_n-Q_n]\n\\]\nこれにより\\(Q_{n+1}\\)は次のように変形できる。\n\\[\nQ_{n+1}=Q_n+\\alpha[R_n-Q_n] \\\\\\\\\n=\\alpha R_n+(1-\\alpha)Q_n \\\\\\\\\n=\\alpha R_n+(1-\\alpha)[\\alpha R_{n-1}+(1-\\alpha)Q_{n-1}] \\\\\\\\\n=\\alpha R_n+(1-\\alpha)\\alpha R_{n-1}+(1-\\alpha)^2Q_{n-1} \\\\\\\\\nこれを繰り返し \\\\\\\\\n=(1-a)^nQ_1+\\sum_{i-1}^{n}\\alpha(1-\\alpha)^{n-i}R_i\n\\]\n\\((1-a)^n+\\sum_{i-1}^{n}\\alpha(1-\\alpha)^{n-i}=1\\)より、\\(Q_{n+1}\\)は過去の報酬と初期の推定値\\(Q_1\\)の加重平均であることが分かる。重みは\\(1-\\alpha\\)の冪乗に従って指数的に減衰していくため、この加重平均を指数直近性加重平均ともいう。\n\\(\\alpha\\)をステップごとに変動させると便利なこともある。行動aがn番目に選択されたときのステップサイズパラメータを\\(\\alpha_n(a)\\)と表す。定常的な問題においては、確率近似理論より、推定行動価値が真の行動価値に確率1で収束する必要十分条件は次の通りである。\n\\[\n\\sum_{n=1}^{\\infty}\\alpha_n(a)=\\infty\\ かつ\\ \\sum_{n=1}^{\\infty}\\alpha_n^2(a)<\\infty\n\\]\nしかし実用では非定常問題を扱う事が多いので、上式を満たすステップサイズパラメータは理論研究では多用されるが、応用や実験研究ではめったに使用されない。\n\n```{python}\nprint(\"Hello Python!\")\n```\n\nHello Python!"
  },
  {
    "objectID": "chapter2.html#楽観的初期値",
    "href": "chapter2.html#楽観的初期値",
    "title": "第２章多腕バンディット問題",
    "section": "楽観的初期値",
    "text": "楽観的初期値\nここまでの手法は初期推定値\\(Q_1(a)\\)にある程度依存する。このことを初期推定値についてバイアスがあるという。\n10本腕バンディットによる実験で、行動価値の初期推定値\\(Q_1(a)\\)を\\(0\\)ではなく\\(+5\\)とする。\\(q_*(a)\\)は標準正規分布から選択されることを考えると楽観的である（\\(q_*(a)\\)が\\(+5\\)以上となる確率は約\\(2.87\\times10^{-7}\\)）。よって最初に行動が選択された後、得られた報酬にエージェントは失望して別の行動をする。このようにして探索を促す初期値を楽観的初期値という。\n10本腕バンディットによる実験で、各\\(Q_1(a)\\)を\\(+5\\)とした場合のグリーディ法と、各\\(Q_1(a)\\)を\\(0\\)とした場合の\\(\\epsilon\\)-グリーディ法の比較を以下に示す。\n\nしかしこの手法も探索は一時的にしか促進されないため、非定常問題にはあまり適さない。"
  },
  {
    "objectID": "chapter2.html#上限信頼区間行動選択",
    "href": "chapter2.html#上限信頼区間行動選択",
    "title": "第２章多腕バンディット問題",
    "section": "上限信頼区間行動選択",
    "text": "上限信頼区間行動選択\n\\(\\epsilon\\)-グリーディ法での探索は行動がランダムが選ばれるが、実際に最適行動である可能性に応じて探索を行うほうが望ましい。すなわち、推定値及びその推定値の不確実性を考慮して選択すれば良い。例えば次式に従って行動することが挙げられる。\n\\[\nA_t:=\\text{arg max}_a\\Bigg(Q_t(a)+c\\sqrt{\\frac{\\log_e t}{N_t(a)}}\\Bigg)\n\\]\n\\(N_t(a)\\)は時刻\\(t\\)までに行動\\(a\\)が選択された回数、\\(c>0\\)は探索を制御する数である。これを上限信頼区間行動選択（UCB）という。UCBを用いた10本腕バンディットによる実験結果を以下に示す。\n 多くの場合でUCBは良い性能を示すが、\\(\\epsilon\\)-グリーディと比べると、一般的な強化学習に拡張して用いることは計算や近似の面において困難であることが多いので、通常は実用的でない。"
  },
  {
    "objectID": "chapter2.html#勾配バンディットアルゴリズム",
    "href": "chapter2.html#勾配バンディットアルゴリズム",
    "title": "第２章多腕バンディット問題",
    "section": "勾配バンディットアルゴリズム",
    "text": "勾配バンディットアルゴリズム\n本章では推定行動価値を行動選択に用いてきた。しかし他にもやり方はある。各行動\\(a\\)に対して数値的に表される優先度（\\(H_t(a)\\)と表す）を学習する方法を考える。行動確率は次のようなソフトマックス分布に従う。\n\\[\n\\text{Pr}\\{A_t=a\\}:=\\pi_t(a):=\\frac{e^{H_t(a)}}{\\sum_{b=1}^{k}e^{H_t(b)}}\n\\]\nここでは時刻\\(t\\)に行動\\(a\\)をとる確率を\\(\\pi_t(a)\\)と表すとする。また初期状態では全ての行動の優先度を同じとする。\n確率的勾配上昇法に基づいて自然に学習アルゴリズムを設定すると、行動\\(A_t\\)で報酬\\(R_t\\)を受け取った後、行動優先度は次のように更新される。\n\\[\nH_{t+1}(A_t)=H_t(A_t)+\\alpha(R_t-\\overline R_t)(1-\\pi_t(A_t)),\\\\\\\\\nH_{t+1}(a):=H_t(A_t)+\\alpha(R_t-\\overline R_t)\\pi(a)\\ \\ ( a\\neq A_t)\n\\]\nただし\\(\\overline R_t\\)（ここではベースラインという）は時刻tまでの全ての報酬の平均である。\n例えばk本腕バンディット問題において、\\(q_*(a)\\)が平均\\(+4\\)、分散\\(1\\)の正規分布から選択されるとする。報酬全体の値が大きくなってもベースラインがそれに応じて増加するので勾配バンディットアルゴリズムでは全く影響しない。しかしベースラインを考慮しない（\\(\\overline R_t\\)=0とする）と次の図からも分かるように性能は大幅に低下する。"
  },
  {
    "objectID": "chapter2.html#連想探索文脈付きバンディット",
    "href": "chapter2.html#連想探索文脈付きバンディット",
    "title": "第２章多腕バンディット問題",
    "section": "連想探索（文脈付きバンディット）",
    "text": "連想探索（文脈付きバンディット）\n第二章では状況に応じて異なる行動をとらせる必要のないような非連想的なタスクのみを考えてきた。しかし一般の強化学習のタスクでは複数の状況があり、方策、すなわちある状態でのそこからとるべき行動への写像を学習することを目標とする。\n例えば、複数のk本腕バンディット問題（タスク）があり、各ステップでランダムに選択されたタスクを行うとする。これは単一の非定常バンディット問題として考えることができるが、これだけだと解を得るのが困難である。そこで各ステップでどのタスクが選ばれたかを示す手がかり（タスクを区別する情報）を得るとする。この場合だとタスクに応じた行動を学習できる。このような問題を連想探索や文脈付きバンディットという。"
  },
  {
    "objectID": "chapter2.html#まとめ",
    "href": "chapter2.html#まとめ",
    "title": "第２章多腕バンディット問題",
    "section": "まとめ",
    "text": "まとめ\nこれまでの手法のk本腕バンディット問題における性能を以下に示す。\\(x\\)軸では各パラメータの値(\\(\\epsilon\\)、\\(\\alpha\\)、\\(c\\)、楽観的初期値\\(Q_0\\))が対数スケールでとられていることに注意。"
  },
  {
    "objectID": "chapter3.html#エージェントと環境の境界",
    "href": "chapter3.html#エージェントと環境の境界",
    "title": "第３章 有限マルコフ決定過程",
    "section": "エージェントと環境の境界",
    "text": "エージェントと環境の境界\n各ステップ\\(t\\)ごとにエージェントは環境の状態\\(S\\)をもとに行動\\(A\\)を選択する。1ステップ後に、行動の結果となる報酬\\(R_{t+1}\\)と次の状態\\(S_{t+1}\\)を獲得する。\n\nマルコフ決定過程では、次に起こる事象の確率が、現在の状態によってのみ決定される。つまり、各状態は将来的な報酬にどれほど影響するのかといった情報を持たなければいけない。これが満たされるとき、マルコフ性を持つといわれる。"
  },
  {
    "objectID": "chapter3.html#目的と報酬",
    "href": "chapter3.html#目的と報酬",
    "title": "第３章 有限マルコフ決定過程",
    "section": "目的と報酬",
    "text": "目的と報酬\n報酬信号は最終目標"
  },
  {
    "objectID": "chapter3.html#収益とエピソード",
    "href": "chapter3.html#収益とエピソード",
    "title": "第３章 有限マルコフ決定過程",
    "section": "収益とエピソード",
    "text": "収益とエピソード"
  },
  {
    "objectID": "chapter3.html#エピソード的タスクと連続タスクの統一的記法",
    "href": "chapter3.html#エピソード的タスクと連続タスクの統一的記法",
    "title": "第３章 有限マルコフ決定過程",
    "section": "エピソード的タスクと連続タスクの統一的記法",
    "text": "エピソード的タスクと連続タスクの統一的記法"
  },
  {
    "objectID": "chapter3.html#方策と価値関数",
    "href": "chapter3.html#方策と価値関数",
    "title": "第３章 有限マルコフ決定過程",
    "section": "方策と価値関数",
    "text": "方策と価値関数"
  }
]