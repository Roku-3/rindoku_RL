---
jupyter: python3
---

# ç¬¬ï¼’ç« å¤šè…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œ

## kæœ¬è…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œ

$k$å€‹ã®é¸æŠè‚¢ãŒã‚ã‚Šã€ã©ã‚Œã‹1ã¤ã‚’é¸ã¶ã¨é¸æŠã«ä¾å­˜ã—ãŸå®šå¸¸ç¢ºç‡åˆ†å¸ƒã‹ã‚‰å ±é…¬ãŒç™ºç”Ÿã™ã‚‹ã€‚ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã—ã€ã‚ã‚‹æ±ºã‚ã‚‰ã‚ŒãŸæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã„ã¦æœŸå¾…åˆè¨ˆå ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹å•é¡Œã‚’$k$æœ¬è…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã¨ã„ã†ã€‚ä»¥å¾Œã€æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—tã§é¸æŠã•ã‚ŒãŸè¡Œå‹•ã‚’$A_t$ã€å¯¾å¿œã™ã‚‹å ±é…¬ã‚’$R_t$ã¨ã™ã‚‹ã€‚$a$ãŒé¸æŠã•ã‚ŒãŸæ™‚ã®ç¢ºç‡å¤‰æ•°ã‚’$X_a$ã¨ã™ã‚‹ã¨ã€$a$ãŒé¸æŠã•ã‚ŒãŸã¨ãã®æœŸå¾…å ±é…¬$q_*(a)$ã‚’æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹ã€‚

$$
q_*(a):=ğ”¼[X_a]
$$

æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—tã®è¡Œå‹•aã®ä¾¡å€¤ã®æ¨å®šå€¤ã‚’$Q_t(a)$ã¨ã™ã‚‹ã€‚ã“ã‚Œã‚’$q_*(a)$ã«è¿‘ã¥ã‘ãŸã„ã€‚ æ¨å®šä¾¡å€¤ãŒæœ€å¤§ã¨ãªã‚‹è¡Œå‹•ã‚’ã‚°ãƒªãƒ¼ãƒ‡ã‚£è¡Œå‹•ã¨ã„ã„ã€çŸ¥è­˜ã‚’**æ´»ç”¨**ã—ã¦ã„ã‚‹ã¨ã„ã†ã€‚ãã†ã§ãªã„ã¨ãã¯**æ¢ç´¢**ã—ã¦ã„ã‚‹ã¨ã„ã†ã€‚

## è¡Œå‹•ä¾¡å€¤æ‰‹æ³•

è¡Œå‹•ä¾¡å€¤ã®æ¨å®šã‚’ã™ã‚‹æ–¹æ³•ã®ç·ç§°ã‚’**è¡Œå‹•ä¾¡å€¤æ‰‹æ³•**ã¨ã„ã†ã€‚è¡Œå‹•ä¾¡å€¤ã‚’æ¨å®šã™ã‚‹è‡ªç„¶ãªæ–¹æ³•ã¨ã—ã¦ã€å®Ÿéš›ã«å¾—ã‚‰ã‚ŒãŸå ±é…¬ã‚’å¹³å‡ã™ã‚‹ã“ã¨ãŒæŒ™ã’ã‚‰ã‚Œã‚‹ã€‚

$$
Q_t(a):= \frac{tã‚ˆã‚Šå‰ã«aã‚’è¡Œã£ãŸã¨ãã®å ±é…¬ã®åˆè¨ˆ}{tã®å‰ã¾ã§ã«aã‚’è¡Œã£ãŸå›æ•°}=\frac{\sum_{i=1}^{t-1}R_iï½¥ğŸ™_{A_i=a}}{\sum_{i=1}^{t-1}ğŸ™_{A_i=a}}
$$ ãŸã ã—$ğŸ™_{æ¡ä»¶}$ã¯æ¡ä»¶ãŒçœŸã®ã¨ã1ã€å½ã®ã¨ã0ã‚’ã¨ã‚‹ã‚‚ã®ã¨ã™ã‚‹ã€‚ã“ã‚Œã‚’è¡Œå‹•ä¾¡å€¤æ¨å®šã®ãŸã‚ã®**ã‚µãƒ³ãƒ—ãƒ«å¹³å‡æ³•**ã¨ã„ã†ã€‚æ¯å›æœ€ã‚‚æ¨å®šä¾¡å€¤ãŒé«˜ã„è¡Œå‹•ã‚’ã™ã‚‹ã‚°ãƒªãƒ¼ãƒ‡ã‚£è¡Œå‹•é¸æŠæ³•ã¯æ¬¡å¼ã§è¡¨ã•ã‚Œã‚‹ã€‚

$$
A_t=\text{arg max}_aQ_t(a)
$$

ãŸã ã—$\text{arg max}_a$ã¯$Q_t(a)$ãŒæœ€å¤§ã¨ãªã‚‹ã‚ˆã†ãª$a$ã‚’ã‹ãˆã™ã€‚ã“ã‚Œã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€æ¢ç´¢ã‚’è¡Œã‚ã›ã‚‹ãŸã‚ã«æ¯å›ç¢ºç‡$\epsilon$ã§ãƒ©ãƒ³ãƒ€ãƒ ãªé¸æŠã‚’è¡Œã‚ã›ã‚‹æ‰‹æ³•ã‚’$\epsilon$**-ã‚°ãƒªãƒ¼ãƒ‡ã‚£æ³•**ã¨ã„ã†ã€‚

## 10æœ¬è…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã«ã‚ˆã‚‹å®Ÿé¨“

2000å€‹ã®kæœ¬è…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã‚’ã€k=10ã¨ã—ã€æ¬¡ã®ã‚ˆã†ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã™ã‚‹ï¼šå„$q_*(a)$ã¯æ¨™æº–æ­£è¦åˆ†å¸ƒ($N(0,1)$)ã«å¾“ã£ã¦ç”Ÿæˆã—ã€å„$X_a$ã¯æ­£è¦åˆ†å¸ƒ$N(q_*(a),1)$ã«å¾“ã†ã¨ã™ã‚‹ã€‚å„å•é¡Œã§ã€1000ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œã‚’1å›ã®**è©¦è¡Œ**ã¨ã™ã‚‹ã€‚

2000å€‹ã®å•é¡Œãã‚Œãã‚Œã«å¯¾ã—ã‚°ãƒªãƒ¼ãƒ‡ã‚£æ³•ã¨äºŒã¤ã®$\epsilon$**-**ã‚°ãƒªãƒ¼ãƒ‡ã‚£æ³•ã‚’è©¦è¡Œã—ã€2000å›ç¹°ã‚Šè¿”ã—å¹³å‡ã‚’ã¨ã£ãŸçµæœã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ã€‚

![](images/c27d33d68b8c4009bf17bf678s246e875-0049.jpg)

å›³ã‹ã‚‰$\epsilon=0.1$ã®ã¨ãã¯$\epsilon=0.01$ã®ã¨ãã‚ˆã‚Šã‚‚ç´ æ—©ãæœ€é©è¡Œå‹•ã‚’ã¨ã‚Œã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ãŒã€è¶…é•·æœŸçš„ãªæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã§ã¿ã‚Œã°æœ€çµ‚çš„ã«ã¯$\epsilon=0.01$ã®ã¨ãã®æ–¹ãŒè‰¯ã„çµæœã‚’ç¤ºã™ã ã‚ã†ã¨ã„ã†ã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚

## é€æ¬¡çš„å®Ÿè£…

ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã®è¨ˆç®—ãŒã©ã®ã‚ˆã†ã«ç°¡ç•¥åŒ–ã§ãã‚‹ã®ã‹ã‚’ç´¹ä»‹ã™ã‚‹ã€‚ç°¡å˜ã®ãŸã‚ã€1æœ¬è…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå•é¡Œã§ã®ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã‚’è€ƒãˆã‚‹ã€‚ã‚¹ãƒ†ãƒƒãƒ—$n$æ™‚ã®æ¨å®šå ±é…¬$Q_n$ã¯æ¬¡å¼ã§è¡¨ã•ã‚Œã‚‹ã€‚

$$
Q_n:=\frac{R_1+R_2+ï½¥ï½¥ï½¥+R_n}{n-1}
$$

å„$R_n$ã‚’è¨˜éŒ²ã—ã¦æ¯å›$Q_n$ã‚’æ±‚ã‚ã‚‹ã‚„ã‚Šæ–¹ã¯å¿…è¦ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—é‡ãŒå¢—å¤§ã—ã¦ã—ã¾ã†ãŒã€å®Ÿéš›ã¯æ¬¡å¼ã®ã‚ˆã†ã«é€ä¸€æ›´æ–°ã§ãã‚‹ã€‚

$$
Q_{n+1}=\frac{1}{n}\sum_{i=1}^{n}R_i\\ =\frac{1}{n}(R_n+\sum_{i=1}^{n-1}R_i)\\ =\frac{1}{n}(R_n+(n-1)Q_n)\\ =Q_n+\frac{1}{n}(R_n-Q_n)
$$

ã“ã†ã™ã‚Œã°ãƒ¡ãƒ¢ãƒªã¯$Q_n$ã¨$n$ã®åˆ†ã ã‘ã§è‰¯ãã€è¨ˆç®—é‡ã‚‚æœ€å¾Œã®å¼ã®ã¿ã§è‰¯ããªã‚‹ã€‚ã“ã‚Œã®ä¸€èˆ¬å½¢ã¯æ¬¡å¼ã§è¡¨ã•ã‚Œã‚‹ã€‚

$$
NewEstimate \leftarrow OldEstimate + StepSize\ [Target - OldEstimate]
$$

$[Target - OldEstimate]$ã¯æ¨å®šã®**èª¤å·®**ã‚’è¡¨ã—ã¦ã„ã‚‹ã€‚ã‚¹ãƒ†ãƒƒãƒ—ãŒé€²ã‚€ã«ã¤ã‚Œæ¨å®šã¯$Target$(ä»Šå›ã¯$n$ç•ªç›®ã®å ±é…¬)ã«è¿‘ã¥ã„ã¦ã„ãã€‚ä»Šå›ã§ã¯$StepSize$ã¯$1/n$ã¨ä¸€å®šã ãŒã€ã‚¹ãƒ†ãƒƒãƒ—ã«å¿œã˜ã¦å¤‰å‹•ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚ä»¥å¾Œã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’$\alpha$ã‚„$\alpha_t(a)$ã¨è¡¨ã™ã€‚

é€æ¬¡çš„è¨ˆç®—ã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«å¹³å‡ã¨$\epsilon$-è¡Œå‹•é¸æŠã‚’ä½¿ã£ãŸkæœ¬è…•ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚‹ã€‚

$$
a=1ã‹ã‚‰kã«ã¤ã„ã¦åˆæœŸåŒ–:\\
Q(a) \leftarrow 0 \\
N(a) \leftarrow 0 \\
\\
æ°¸ä¹…ã«ç¹°ã‚Šè¿”ã—:\\
A \leftarrow 
\begin{cases}
    \text{arg max}_aQ(a)\ \ \ ç¢ºç‡1-\epsilonã§\ \ (åŒé †ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ) \\
    ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•\ \ \ ç¢ºç‡\epsilonã§
\end{cases}\\
R \leftarrow bandit(A)\\
N(A) \leftarrow N(A)+1\\
Q(A) \leftarrow Q(A) + \frac{1}{N(A)}[R-Q(A)]
$$

## éå®šå¸¸å•é¡Œã‚’èª¿ã¹ã‚‹

éå®šå¸¸çš„ãªå•é¡Œã€ã™ãªã‚ã¡ã€è¡Œå‹•ã«å¯¾ã™ã‚‹å ±é…¬(ã¤ã¾ã‚Š$X_a$)ãŒæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å¤‰åŒ–ã™ã‚‹å•é¡Œã®å ´åˆã«ã¤ã„ã¦ã¯,ã€æ–°ã—ã„å ±é…¬ã®æ–¹ã«é‡ã¿ã‚’ã¤ã‘ã‚‹æ–¹ãŒç†ã«ã‹ãªã£ã¦ã„ã‚‹ã€‚ãã“ã§ã€é€æ¬¡æ›´æ–°å‰‡ã¯å®šæ•°$\alpha \in (0,1]$ã‚’ç”¨ã„ã¦æ¬¡ã®ã‚ˆã†ã«ä¿®æ­£ã•ã‚Œã‚‹ã€‚

$$
Q_{n+1}:=Q_n+\alpha[R_n-Q_n]
$$ ã“ã‚Œã«ã‚ˆã‚Š$Q_{n+1}$ã¯æ¬¡ã®ã‚ˆã†ã«å¤‰å½¢ã§ãã‚‹ã€‚ $$
Q_{n+1}=Q_n+\alpha[R_n-Q_n]\\
=\alpha R_n+(1-\alpha)Q_n\\
=\alpha R_n+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}]\\
=\alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^2Q_{n-1}\\
ã“ã‚Œã‚’ç¹°ã‚Šè¿”ã—\\
=(1-a)^nQ_1+\sum_{i-1}^{n}\alpha(1-\alpha)^{n-i}R_i
$$

$(1-a)^n+\sum_{i-1}^{n}\alpha(1-\alpha)^{n-i}=1$ã‚ˆã‚Šã€$Q_{n+1}$ã¯éå»ã®å ±é…¬ã¨åˆæœŸã®æ¨å®šå€¤$Q_1$ã®åŠ é‡å¹³å‡ã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚é‡ã¿ã¯$1-\alpha$ã®å†ªä¹—ã«å¾“ã£ã¦æŒ‡æ•°çš„ã«æ¸›è¡°ã—ã¦ã„ããŸã‚ã€ã“ã®åŠ é‡å¹³å‡ã‚’**æŒ‡æ•°ç›´è¿‘æ€§åŠ é‡å¹³å‡**ã¨ã‚‚ã„ã†ã€‚

$\alpha$ã‚’ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å¤‰å‹•ã•ã›ã‚‹ã¨ä¾¿åˆ©ãªã“ã¨ã‚‚ã‚ã‚‹ã€‚è¡Œå‹•aãŒnç•ªç›®ã«é¸æŠã•ã‚ŒãŸã¨ãã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’$\alpha_n(a)$ã¨è¡¨ã™ã€‚å®šå¸¸çš„ãªå•é¡Œã«ãŠã„ã¦ã¯ã€ç¢ºç‡è¿‘ä¼¼ç†è«–ã‚ˆã‚Šã€æ¨å®šè¡Œå‹•ä¾¡å€¤ãŒçœŸã®è¡Œå‹•ä¾¡å€¤ã«ç¢ºç‡1ã§åæŸã™ã‚‹å¿…è¦ååˆ†æ¡ä»¶ã¯æ¬¡ã®é€šã‚Šã§ã‚ã‚‹ã€‚

$$
\sum_{n=1}^{\infty}\alpha_n(a)=\infty\ ã‹ã¤\ \sum_{n=1}^{\infty}\alpha_n^2(a)<\infty 
$$

ã—ã‹ã—å®Ÿç”¨ã§ã¯éå®šå¸¸å•é¡Œã‚’æ‰±ã†äº‹ãŒå¤šã„ã®ã§ã€ä¸Šå¼ã‚’æº€ãŸã™ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç†è«–ç ”ç©¶ã§ã¯å¤šç”¨ã•ã‚Œã‚‹ãŒã€å¿œç”¨ã‚„å®Ÿé¨“ç ”ç©¶ã§ã¯ã‚ã£ãŸã«ä½¿ç”¨ã•ã‚Œãªã„ã€‚

```{python}
print("Hello Python!")
```

```{python}
# -*- coding:utf-8 -*-
import numpy as np
import random
 
class BernoulliArm():
 
    def __init__(self, p):
        self.p = p
 
    def draw(self):
        if random.random() > self.p:
            return 0.0
        else:
            return 1.0
 
class random_select():
 
    def __init__(self, counts, values):
        self.counts = counts
        self.values = values
 
    def initialize(self, n_arms):
        self.counts = np.zeros(n_arms)
        self.values = np.zeros(n_arms)
 
    def select_arm(self):
        return random.randint(0, len(self.values) - 1)
 
    def update(self, chosen_arm, reward):
        self.counts[chosen_arm] = self.counts[chosen_arm] + 1
        n = self.counts[chosen_arm]
        value = self.values[chosen_arm]
        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward
        self.values[chosen_arm] = new_value
 
class EpsilonGreedy():
 
    def __init__(self, epsilon, counts, values):
        self.epsilon = epsilon
        self.counts = counts
        self.values = values
 
    def initialize(self, n_arms):
        self.counts = np.zeros(n_arms)
        self.values = np.zeros(n_arms)
 
    def select_arm(self):
        if random.random() > self.epsilon:
            return np.argmax(self.values)
        else:
            return random.randint(0, len(self.values) - 1)
 
    def update(self, chosen_arm, reward):
        self.counts[chosen_arm] = self.counts[chosen_arm] + 1
        n = self.counts[chosen_arm]
        value = self.values[chosen_arm]
        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward
        self.values[chosen_arm] = new_value
 
class UCB():
 
    def __init__(self, counts, values):
        self.counts = counts
        self.values = values
 
    def initialize(self, n_arms):
        self.counts = np.zeros(n_arms)
        self.values = np.zeros(n_arms)
 
    def select_arm(self):
        n_arms = len(self.counts)
        if min(self.counts) == 0:
            return np.argmin(self.counts)
 
        total_counts = sum(self.counts)
        bonus = np.sqrt((np.log(np.array(total_counts))) /
                        2 / np.array(self.counts))
        ucb_values = np.array(self.values) + bonus
        return np.argmax(ucb_values)
 
    def update(self, chosen_arm, reward):
        self.counts[chosen_arm] = self.counts[chosen_arm] + 1
        n = self.counts[chosen_arm]
        value = self.values[chosen_arm]
        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward
        self.values[chosen_arm] = new_value
 
class ThompsonSampling():
 
    def __init__(self, counts_alpha, counts_beta, values):
        self.counts_alpha = counts_alpha
        self.counts_beta = counts_beta
        self.alpha = 1
        self.beta = 1
        self.values = values
 
    def initialize(self, n_arms):
        self.counts_alpha = np.zeros(n_arms)
        self.counts_beta = np.zeros(n_arms)
        self.values = np.zeros(n_arms)
 
    def select_arm(self):
        theta = [(arm,
                  random.betavariate(self.counts_alpha[arm] + self.alpha,
                                     self.counts_beta[arm] + self.beta))
                 for arm in range(len(self.counts_alpha))]
        theta = sorted(theta, key=lambda x: x[1])
        return theta[-1][0]
 
    def update(self, chosen_arm, reward):
        if reward == 1:
            self.counts_alpha[chosen_arm] += 1
        else:
            self.counts_beta[chosen_arm] += 1
        n = float(self.counts_alpha[chosen_arm]) + self.counts_beta[chosen_arm]
        self.values[chosen_arm] = (n - 1) / n * \
            self.values[chosen_arm] + 1 / n * reward
 
def test_algorithm(algo, arms, num_sims, horizon):
    chosen_arms = np.zeros(num_sims * horizon)
    cumulative_rewards = np.zeros(num_sims * horizon)
    times = np.zeros(num_sims * horizon)
    for sim in range(num_sims):
        algo.initialize(len(arms))
        for t in range(horizon):
            index = sim * horizon + t
            times[index] = t + 1
            chosen_arm = algo.select_arm()
            chosen_arms[index] = chosen_arm
            reward = arms[chosen_arm].draw()
            if t == 0:
                cumulative_rewards[index] = reward
            else:
                cumulative_rewards[index] = cumulative_rewards[
                    index - 1] + reward
            algo.update(chosen_arm, reward)
    return [times, chosen_arms, cumulative_rewards]
```

```{python}
import numpy as np
import matplotlib.pyplot as plt

# è©¦è¡Œå›æ•°ã‚’æŒ‡å®š
steps = 1000

# ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚·ãƒ³ã‚’é¸ã¶ç¢ºç‡ã‚’æŒ‡å®š
epsilon = 0.1

# ãƒã‚·ãƒ³ã®æ•°ã‚’æŒ‡å®š
action_size = 10

# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
bandit = Bandit(action_size)
agent = Agent(epsilon, action_size)

# ç·å ±é…¬ã‚’åˆæœŸåŒ–
total_reward = 0

# è¨˜éŒ²ç”¨ã®ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–
total_rewards = []
rates = []
all_rates = np.zeros((action_size, steps+1))

# åˆæœŸå€¤ã‚’è¨˜éŒ²
all_rates[:, 0] = agent.Qs

# ç¹°ã‚Šè¿”ã—è©¦è¡Œ
for step in range(steps):
    # Îµ-greedyæ³•ã«ã‚ˆã‚Šãƒ—ãƒ¬ã‚¤ã™ã‚‹ãƒã‚·ãƒ³ç•ªå·ã‚’æ±ºå®š
    action = agent.get_action()
    
    # actionç•ªç›®ã®ãƒã‚·ãƒ³ã‚’ãƒ—ãƒ¬ã‚¤
    reward = bandit.play(action)
    
    # actionç•ªç›®ã®ãƒã‚·ãƒ³ã®æ¨å®šä¾¡å€¤ã‚’æ›´æ–°
    agent.update(action, reward)
    
    # å ±é…¬ã‚’åŠ ç®—
    total_reward += reward
    
    # æ›´æ–°å€¤ã‚’è¨˜éŒ²
    total_rewards.append(total_reward)
    rates.append(total_reward / (step + 1))
    all_rates[:, step+1] = agent.Qs

# æœ€çµ‚çµæœã‚’ç¢ºèª
print(total_reward)
print(rates[steps-1])


```

``` [python]
```
